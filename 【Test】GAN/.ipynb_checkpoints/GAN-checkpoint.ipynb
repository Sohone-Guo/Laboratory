{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.661930, acc.: 65.62%] [G loss: 0.623236]\n",
      "1 [D loss: 0.362542, acc.: 81.25%] [G loss: 0.606612]\n",
      "2 [D loss: 0.338977, acc.: 84.38%] [G loss: 0.698047]\n",
      "3 [D loss: 0.351163, acc.: 78.12%] [G loss: 0.748897]\n",
      "4 [D loss: 0.318728, acc.: 87.50%] [G loss: 0.895619]\n",
      "5 [D loss: 0.316124, acc.: 84.38%] [G loss: 0.962243]\n",
      "6 [D loss: 0.249772, acc.: 93.75%] [G loss: 1.088366]\n",
      "7 [D loss: 0.236180, acc.: 96.88%] [G loss: 1.296191]\n",
      "8 [D loss: 0.186298, acc.: 100.00%] [G loss: 1.521891]\n",
      "9 [D loss: 0.165513, acc.: 100.00%] [G loss: 1.606403]\n",
      "10 [D loss: 0.133421, acc.: 100.00%] [G loss: 1.756692]\n",
      "11 [D loss: 0.097568, acc.: 100.00%] [G loss: 1.914915]\n",
      "12 [D loss: 0.098718, acc.: 100.00%] [G loss: 2.097588]\n",
      "13 [D loss: 0.094844, acc.: 100.00%] [G loss: 2.209769]\n",
      "14 [D loss: 0.073803, acc.: 100.00%] [G loss: 2.314499]\n",
      "15 [D loss: 0.068275, acc.: 100.00%] [G loss: 2.420779]\n",
      "16 [D loss: 0.068661, acc.: 100.00%] [G loss: 2.455841]\n",
      "17 [D loss: 0.038719, acc.: 100.00%] [G loss: 2.533018]\n",
      "18 [D loss: 0.040643, acc.: 100.00%] [G loss: 2.609970]\n",
      "19 [D loss: 0.067052, acc.: 100.00%] [G loss: 2.599045]\n",
      "20 [D loss: 0.045410, acc.: 100.00%] [G loss: 2.894344]\n",
      "21 [D loss: 0.036781, acc.: 100.00%] [G loss: 3.030809]\n",
      "22 [D loss: 0.041691, acc.: 100.00%] [G loss: 2.938119]\n",
      "23 [D loss: 0.034099, acc.: 100.00%] [G loss: 3.028593]\n",
      "24 [D loss: 0.036363, acc.: 100.00%] [G loss: 3.152449]\n",
      "25 [D loss: 0.033026, acc.: 100.00%] [G loss: 3.027320]\n",
      "26 [D loss: 0.031709, acc.: 100.00%] [G loss: 3.233383]\n",
      "27 [D loss: 0.028876, acc.: 100.00%] [G loss: 3.193343]\n",
      "28 [D loss: 0.023547, acc.: 100.00%] [G loss: 3.364328]\n",
      "29 [D loss: 0.025939, acc.: 100.00%] [G loss: 3.348262]\n",
      "30 [D loss: 0.024032, acc.: 100.00%] [G loss: 3.312400]\n",
      "31 [D loss: 0.019439, acc.: 100.00%] [G loss: 3.330664]\n",
      "32 [D loss: 0.013454, acc.: 100.00%] [G loss: 3.348516]\n",
      "33 [D loss: 0.027392, acc.: 100.00%] [G loss: 3.453960]\n",
      "34 [D loss: 0.034965, acc.: 100.00%] [G loss: 3.529245]\n",
      "35 [D loss: 0.022898, acc.: 100.00%] [G loss: 3.549333]\n",
      "36 [D loss: 0.020230, acc.: 100.00%] [G loss: 3.649617]\n",
      "37 [D loss: 0.018465, acc.: 100.00%] [G loss: 3.677889]\n",
      "38 [D loss: 0.023639, acc.: 100.00%] [G loss: 3.760812]\n",
      "39 [D loss: 0.022677, acc.: 100.00%] [G loss: 3.755875]\n",
      "40 [D loss: 0.017673, acc.: 100.00%] [G loss: 3.841653]\n",
      "41 [D loss: 0.017881, acc.: 100.00%] [G loss: 3.820729]\n",
      "42 [D loss: 0.015130, acc.: 100.00%] [G loss: 3.882883]\n",
      "43 [D loss: 0.017144, acc.: 100.00%] [G loss: 3.821299]\n",
      "44 [D loss: 0.012886, acc.: 100.00%] [G loss: 3.776989]\n",
      "45 [D loss: 0.013766, acc.: 100.00%] [G loss: 3.801589]\n",
      "46 [D loss: 0.020756, acc.: 100.00%] [G loss: 3.902082]\n",
      "47 [D loss: 0.013000, acc.: 100.00%] [G loss: 3.801457]\n",
      "48 [D loss: 0.014592, acc.: 100.00%] [G loss: 3.823703]\n",
      "49 [D loss: 0.019439, acc.: 100.00%] [G loss: 3.899423]\n",
      "50 [D loss: 0.017548, acc.: 100.00%] [G loss: 3.877329]\n",
      "51 [D loss: 0.014854, acc.: 100.00%] [G loss: 3.910992]\n",
      "52 [D loss: 0.018112, acc.: 100.00%] [G loss: 3.980652]\n",
      "53 [D loss: 0.010933, acc.: 100.00%] [G loss: 3.889983]\n",
      "54 [D loss: 0.021926, acc.: 100.00%] [G loss: 4.001202]\n",
      "55 [D loss: 0.008771, acc.: 100.00%] [G loss: 4.069718]\n",
      "56 [D loss: 0.017288, acc.: 100.00%] [G loss: 4.075208]\n",
      "57 [D loss: 0.013597, acc.: 100.00%] [G loss: 4.165188]\n",
      "58 [D loss: 0.016462, acc.: 100.00%] [G loss: 4.293332]\n",
      "59 [D loss: 0.014094, acc.: 100.00%] [G loss: 4.200482]\n",
      "60 [D loss: 0.013398, acc.: 100.00%] [G loss: 4.037090]\n",
      "61 [D loss: 0.011406, acc.: 100.00%] [G loss: 4.045088]\n",
      "62 [D loss: 0.011103, acc.: 100.00%] [G loss: 4.159536]\n",
      "63 [D loss: 0.015064, acc.: 100.00%] [G loss: 4.201054]\n",
      "64 [D loss: 0.022521, acc.: 100.00%] [G loss: 4.231874]\n",
      "65 [D loss: 0.014003, acc.: 100.00%] [G loss: 4.281560]\n",
      "66 [D loss: 0.013441, acc.: 100.00%] [G loss: 4.216515]\n",
      "67 [D loss: 0.010151, acc.: 100.00%] [G loss: 4.318066]\n",
      "68 [D loss: 0.015493, acc.: 100.00%] [G loss: 4.329761]\n",
      "69 [D loss: 0.006648, acc.: 100.00%] [G loss: 4.256133]\n",
      "70 [D loss: 0.011879, acc.: 100.00%] [G loss: 4.235404]\n",
      "71 [D loss: 0.009579, acc.: 100.00%] [G loss: 4.362958]\n",
      "72 [D loss: 0.014204, acc.: 100.00%] [G loss: 4.393123]\n",
      "73 [D loss: 0.020633, acc.: 100.00%] [G loss: 4.500567]\n",
      "74 [D loss: 0.014190, acc.: 100.00%] [G loss: 4.315344]\n",
      "75 [D loss: 0.009090, acc.: 100.00%] [G loss: 4.288216]\n",
      "76 [D loss: 0.021348, acc.: 100.00%] [G loss: 4.375555]\n",
      "77 [D loss: 0.011780, acc.: 100.00%] [G loss: 4.551462]\n",
      "78 [D loss: 0.016081, acc.: 100.00%] [G loss: 4.557851]\n",
      "79 [D loss: 0.007140, acc.: 100.00%] [G loss: 4.669101]\n",
      "80 [D loss: 0.012549, acc.: 100.00%] [G loss: 4.473340]\n",
      "81 [D loss: 0.016768, acc.: 100.00%] [G loss: 4.467517]\n",
      "82 [D loss: 0.017813, acc.: 100.00%] [G loss: 4.436097]\n",
      "83 [D loss: 0.007334, acc.: 100.00%] [G loss: 4.518083]\n",
      "84 [D loss: 0.007225, acc.: 100.00%] [G loss: 4.461599]\n",
      "85 [D loss: 0.015618, acc.: 100.00%] [G loss: 4.406104]\n",
      "86 [D loss: 0.014906, acc.: 100.00%] [G loss: 4.464816]\n",
      "87 [D loss: 0.008116, acc.: 100.00%] [G loss: 4.480640]\n",
      "88 [D loss: 0.015203, acc.: 100.00%] [G loss: 4.594709]\n",
      "89 [D loss: 0.008655, acc.: 100.00%] [G loss: 4.821000]\n",
      "90 [D loss: 0.009325, acc.: 100.00%] [G loss: 4.766980]\n",
      "91 [D loss: 0.016537, acc.: 100.00%] [G loss: 4.567492]\n",
      "92 [D loss: 0.008653, acc.: 100.00%] [G loss: 4.696504]\n",
      "93 [D loss: 0.010379, acc.: 100.00%] [G loss: 4.767617]\n",
      "94 [D loss: 0.012337, acc.: 100.00%] [G loss: 4.627347]\n",
      "95 [D loss: 0.013493, acc.: 100.00%] [G loss: 4.702270]\n",
      "96 [D loss: 0.008158, acc.: 100.00%] [G loss: 4.815207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 [D loss: 0.015965, acc.: 100.00%] [G loss: 4.766818]\n",
      "98 [D loss: 0.013484, acc.: 100.00%] [G loss: 4.812083]\n",
      "99 [D loss: 0.010489, acc.: 100.00%] [G loss: 4.746594]\n",
      "100 [D loss: 0.008062, acc.: 100.00%] [G loss: 4.641602]\n",
      "101 [D loss: 0.018125, acc.: 100.00%] [G loss: 4.998999]\n",
      "102 [D loss: 0.012566, acc.: 100.00%] [G loss: 4.861823]\n",
      "103 [D loss: 0.019380, acc.: 100.00%] [G loss: 5.045329]\n",
      "104 [D loss: 0.010483, acc.: 100.00%] [G loss: 4.880792]\n",
      "105 [D loss: 0.015628, acc.: 100.00%] [G loss: 5.004563]\n",
      "106 [D loss: 0.013592, acc.: 100.00%] [G loss: 4.987072]\n",
      "107 [D loss: 0.018321, acc.: 100.00%] [G loss: 4.843178]\n",
      "108 [D loss: 0.016092, acc.: 100.00%] [G loss: 4.894298]\n",
      "109 [D loss: 0.008290, acc.: 100.00%] [G loss: 5.199981]\n",
      "110 [D loss: 0.015811, acc.: 100.00%] [G loss: 4.970630]\n",
      "111 [D loss: 0.020273, acc.: 100.00%] [G loss: 5.276041]\n",
      "112 [D loss: 0.017002, acc.: 100.00%] [G loss: 5.159397]\n",
      "113 [D loss: 0.008253, acc.: 100.00%] [G loss: 4.901512]\n",
      "114 [D loss: 0.019811, acc.: 100.00%] [G loss: 5.085690]\n",
      "115 [D loss: 0.019336, acc.: 100.00%] [G loss: 5.039227]\n",
      "116 [D loss: 0.023143, acc.: 100.00%] [G loss: 5.126081]\n",
      "117 [D loss: 0.015847, acc.: 100.00%] [G loss: 5.204130]\n",
      "118 [D loss: 0.027825, acc.: 100.00%] [G loss: 5.217231]\n",
      "119 [D loss: 0.025293, acc.: 100.00%] [G loss: 5.052059]\n",
      "120 [D loss: 0.016676, acc.: 100.00%] [G loss: 5.214455]\n",
      "121 [D loss: 0.063865, acc.: 100.00%] [G loss: 4.657617]\n",
      "122 [D loss: 0.012454, acc.: 100.00%] [G loss: 4.859829]\n",
      "123 [D loss: 0.023531, acc.: 100.00%] [G loss: 5.274184]\n",
      "124 [D loss: 0.032568, acc.: 100.00%] [G loss: 6.120687]\n",
      "125 [D loss: 0.221356, acc.: 90.62%] [G loss: 4.226690]\n",
      "126 [D loss: 0.020863, acc.: 100.00%] [G loss: 4.665439]\n",
      "127 [D loss: 0.067820, acc.: 96.88%] [G loss: 5.516978]\n",
      "128 [D loss: 0.041050, acc.: 100.00%] [G loss: 4.873947]\n",
      "129 [D loss: 0.026773, acc.: 100.00%] [G loss: 4.716020]\n",
      "130 [D loss: 0.046761, acc.: 100.00%] [G loss: 5.657474]\n",
      "131 [D loss: 0.515988, acc.: 84.38%] [G loss: 3.292166]\n",
      "132 [D loss: 0.114361, acc.: 93.75%] [G loss: 4.700088]\n",
      "133 [D loss: 0.011732, acc.: 100.00%] [G loss: 5.368324]\n",
      "134 [D loss: 0.043885, acc.: 100.00%] [G loss: 4.661672]\n",
      "135 [D loss: 0.063457, acc.: 96.88%] [G loss: 5.063047]\n",
      "136 [D loss: 0.044929, acc.: 100.00%] [G loss: 4.561494]\n",
      "137 [D loss: 0.067348, acc.: 96.88%] [G loss: 4.841429]\n",
      "138 [D loss: 0.158627, acc.: 90.62%] [G loss: 3.315985]\n",
      "139 [D loss: 0.143383, acc.: 93.75%] [G loss: 3.835464]\n",
      "140 [D loss: 0.026058, acc.: 100.00%] [G loss: 5.050224]\n",
      "141 [D loss: 0.008732, acc.: 100.00%] [G loss: 4.749279]\n",
      "142 [D loss: 0.075789, acc.: 96.88%] [G loss: 4.183230]\n",
      "143 [D loss: 0.068202, acc.: 96.88%] [G loss: 5.290941]\n",
      "144 [D loss: 0.205743, acc.: 90.62%] [G loss: 4.989812]\n",
      "145 [D loss: 0.057773, acc.: 100.00%] [G loss: 5.504236]\n",
      "146 [D loss: 0.239935, acc.: 87.50%] [G loss: 3.173687]\n",
      "147 [D loss: 0.472189, acc.: 84.38%] [G loss: 4.700715]\n",
      "148 [D loss: 0.015294, acc.: 100.00%] [G loss: 6.555830]\n",
      "149 [D loss: 1.856450, acc.: 34.38%] [G loss: 1.742686]\n",
      "150 [D loss: 0.712220, acc.: 71.88%] [G loss: 2.145636]\n",
      "151 [D loss: 0.558658, acc.: 81.25%] [G loss: 2.932087]\n",
      "152 [D loss: 0.040747, acc.: 100.00%] [G loss: 3.486357]\n",
      "153 [D loss: 0.109934, acc.: 93.75%] [G loss: 3.929217]\n",
      "154 [D loss: 0.151614, acc.: 90.62%] [G loss: 4.156296]\n",
      "155 [D loss: 0.093714, acc.: 96.88%] [G loss: 3.781766]\n",
      "156 [D loss: 0.041743, acc.: 100.00%] [G loss: 3.733461]\n",
      "157 [D loss: 0.278881, acc.: 84.38%] [G loss: 3.431927]\n",
      "158 [D loss: 0.100953, acc.: 100.00%] [G loss: 3.745661]\n",
      "159 [D loss: 0.561502, acc.: 71.88%] [G loss: 2.249542]\n",
      "160 [D loss: 0.286232, acc.: 81.25%] [G loss: 3.082754]\n",
      "161 [D loss: 0.127956, acc.: 96.88%] [G loss: 3.474501]\n",
      "162 [D loss: 0.385349, acc.: 81.25%] [G loss: 1.659741]\n",
      "163 [D loss: 0.192823, acc.: 93.75%] [G loss: 1.915478]\n",
      "164 [D loss: 0.101357, acc.: 96.88%] [G loss: 2.827833]\n",
      "165 [D loss: 0.108834, acc.: 96.88%] [G loss: 3.248633]\n",
      "166 [D loss: 0.091769, acc.: 100.00%] [G loss: 2.939874]\n",
      "167 [D loss: 0.200341, acc.: 90.62%] [G loss: 2.576161]\n",
      "168 [D loss: 0.163536, acc.: 93.75%] [G loss: 2.814982]\n",
      "169 [D loss: 0.291107, acc.: 87.50%] [G loss: 2.562546]\n",
      "170 [D loss: 0.206163, acc.: 90.62%] [G loss: 3.420821]\n",
      "171 [D loss: 0.323495, acc.: 87.50%] [G loss: 1.757928]\n",
      "172 [D loss: 0.478747, acc.: 71.88%] [G loss: 3.243092]\n",
      "173 [D loss: 0.087372, acc.: 100.00%] [G loss: 4.278651]\n",
      "174 [D loss: 0.510373, acc.: 78.12%] [G loss: 1.729971]\n",
      "175 [D loss: 0.260054, acc.: 87.50%] [G loss: 2.946410]\n",
      "176 [D loss: 0.118376, acc.: 96.88%] [G loss: 3.311738]\n",
      "177 [D loss: 0.103124, acc.: 100.00%] [G loss: 3.450770]\n",
      "178 [D loss: 0.202038, acc.: 93.75%] [G loss: 2.401043]\n",
      "179 [D loss: 0.219151, acc.: 93.75%] [G loss: 3.317996]\n",
      "180 [D loss: 0.187569, acc.: 90.62%] [G loss: 3.234322]\n",
      "181 [D loss: 0.143595, acc.: 96.88%] [G loss: 2.492303]\n",
      "182 [D loss: 0.084429, acc.: 96.88%] [G loss: 2.830663]\n",
      "183 [D loss: 0.159525, acc.: 93.75%] [G loss: 3.753192]\n",
      "184 [D loss: 0.267420, acc.: 87.50%] [G loss: 2.389014]\n",
      "185 [D loss: 0.150621, acc.: 90.62%] [G loss: 3.317596]\n",
      "186 [D loss: 0.293217, acc.: 87.50%] [G loss: 2.701482]\n",
      "187 [D loss: 0.151617, acc.: 90.62%] [G loss: 3.662655]\n",
      "188 [D loss: 0.506420, acc.: 75.00%] [G loss: 2.221298]\n",
      "189 [D loss: 0.197085, acc.: 84.38%] [G loss: 4.527504]\n",
      "190 [D loss: 0.308910, acc.: 87.50%] [G loss: 2.564667]\n",
      "191 [D loss: 0.181795, acc.: 90.62%] [G loss: 3.055725]\n",
      "192 [D loss: 0.076291, acc.: 100.00%] [G loss: 3.754302]\n",
      "193 [D loss: 0.476278, acc.: 78.12%] [G loss: 1.272383]\n",
      "194 [D loss: 0.485944, acc.: 71.88%] [G loss: 2.876657]\n",
      "195 [D loss: 0.119282, acc.: 93.75%] [G loss: 3.176113]\n",
      "196 [D loss: 0.355785, acc.: 87.50%] [G loss: 2.705232]\n",
      "197 [D loss: 0.133574, acc.: 93.75%] [G loss: 3.560994]\n",
      "198 [D loss: 0.433459, acc.: 75.00%] [G loss: 1.818771]\n",
      "199 [D loss: 0.184396, acc.: 90.62%] [G loss: 3.132073]\n",
      "200 [D loss: 0.146889, acc.: 96.88%] [G loss: 3.894094]\n",
      "201 [D loss: 0.726575, acc.: 56.25%] [G loss: 1.451461]\n",
      "202 [D loss: 0.157698, acc.: 96.88%] [G loss: 3.066504]\n",
      "203 [D loss: 0.317493, acc.: 93.75%] [G loss: 2.325259]\n",
      "204 [D loss: 0.166032, acc.: 96.88%] [G loss: 3.054373]\n",
      "205 [D loss: 0.257262, acc.: 90.62%] [G loss: 3.221565]\n",
      "206 [D loss: 0.221485, acc.: 93.75%] [G loss: 2.254417]\n",
      "207 [D loss: 0.246234, acc.: 90.62%] [G loss: 3.579401]\n",
      "208 [D loss: 0.389551, acc.: 81.25%] [G loss: 2.315478]\n",
      "209 [D loss: 0.350103, acc.: 81.25%] [G loss: 3.294242]\n",
      "210 [D loss: 0.163502, acc.: 96.88%] [G loss: 2.769508]\n",
      "211 [D loss: 0.320309, acc.: 87.50%] [G loss: 2.190624]\n",
      "212 [D loss: 0.195521, acc.: 100.00%] [G loss: 2.502665]\n",
      "213 [D loss: 0.419670, acc.: 78.12%] [G loss: 2.775956]\n",
      "214 [D loss: 0.582915, acc.: 68.75%] [G loss: 2.965108]\n",
      "215 [D loss: 0.678336, acc.: 56.25%] [G loss: 1.897649]\n",
      "216 [D loss: 0.173689, acc.: 100.00%] [G loss: 3.888507]\n",
      "217 [D loss: 1.440320, acc.: 21.88%] [G loss: 0.511252]\n",
      "218 [D loss: 0.636703, acc.: 56.25%] [G loss: 2.596980]\n",
      "219 [D loss: 0.387410, acc.: 81.25%] [G loss: 3.464303]\n",
      "220 [D loss: 0.764424, acc.: 56.25%] [G loss: 0.920166]\n",
      "221 [D loss: 0.453179, acc.: 62.50%] [G loss: 2.502036]\n",
      "222 [D loss: 0.623618, acc.: 59.38%] [G loss: 1.680747]\n",
      "223 [D loss: 0.354426, acc.: 90.62%] [G loss: 2.424120]\n",
      "224 [D loss: 0.420222, acc.: 75.00%] [G loss: 1.776008]\n",
      "225 [D loss: 0.564289, acc.: 65.62%] [G loss: 1.657235]\n",
      "226 [D loss: 0.501766, acc.: 71.88%] [G loss: 1.913283]\n",
      "227 [D loss: 0.602915, acc.: 62.50%] [G loss: 2.214870]\n",
      "228 [D loss: 0.559293, acc.: 65.62%] [G loss: 1.779476]\n",
      "229 [D loss: 0.519673, acc.: 68.75%] [G loss: 2.341918]\n",
      "230 [D loss: 0.598225, acc.: 62.50%] [G loss: 1.110238]\n",
      "231 [D loss: 0.481768, acc.: 62.50%] [G loss: 2.351140]\n",
      "232 [D loss: 1.360562, acc.: 18.75%] [G loss: 0.669603]\n",
      "233 [D loss: 0.551077, acc.: 65.62%] [G loss: 1.972850]\n",
      "234 [D loss: 1.189303, acc.: 31.25%] [G loss: 0.777781]\n",
      "235 [D loss: 0.431477, acc.: 75.00%] [G loss: 1.705492]\n",
      "236 [D loss: 0.916896, acc.: 37.50%] [G loss: 0.780295]\n",
      "237 [D loss: 0.578047, acc.: 68.75%] [G loss: 1.203068]\n",
      "238 [D loss: 0.500782, acc.: 84.38%] [G loss: 1.386227]\n",
      "239 [D loss: 0.878869, acc.: 37.50%] [G loss: 0.632855]\n",
      "240 [D loss: 0.477090, acc.: 68.75%] [G loss: 1.656293]\n",
      "241 [D loss: 0.942428, acc.: 28.12%] [G loss: 0.888776]\n",
      "242 [D loss: 0.626504, acc.: 53.12%] [G loss: 1.254313]\n",
      "243 [D loss: 0.623206, acc.: 59.38%] [G loss: 1.074999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 [D loss: 0.676029, acc.: 56.25%] [G loss: 1.224958]\n",
      "245 [D loss: 1.037067, acc.: 31.25%] [G loss: 0.765532]\n",
      "246 [D loss: 0.673105, acc.: 50.00%] [G loss: 0.985473]\n",
      "247 [D loss: 0.724925, acc.: 46.88%] [G loss: 1.077056]\n",
      "248 [D loss: 0.769787, acc.: 40.62%] [G loss: 0.929680]\n",
      "249 [D loss: 0.680495, acc.: 46.88%] [G loss: 1.033176]\n",
      "250 [D loss: 0.733438, acc.: 50.00%] [G loss: 0.982123]\n",
      "251 [D loss: 0.681748, acc.: 53.12%] [G loss: 0.946832]\n",
      "252 [D loss: 0.809787, acc.: 37.50%] [G loss: 0.792056]\n",
      "253 [D loss: 0.607527, acc.: 59.38%] [G loss: 1.004329]\n",
      "254 [D loss: 0.849763, acc.: 31.25%] [G loss: 0.732867]\n",
      "255 [D loss: 0.658774, acc.: 46.88%] [G loss: 0.773408]\n",
      "256 [D loss: 0.684575, acc.: 43.75%] [G loss: 0.747482]\n",
      "257 [D loss: 0.721245, acc.: 46.88%] [G loss: 0.816052]\n",
      "258 [D loss: 0.752282, acc.: 43.75%] [G loss: 0.777599]\n",
      "259 [D loss: 0.564403, acc.: 68.75%] [G loss: 1.112757]\n",
      "260 [D loss: 0.827266, acc.: 37.50%] [G loss: 0.813205]\n",
      "261 [D loss: 0.777882, acc.: 46.88%] [G loss: 0.722068]\n",
      "262 [D loss: 0.683694, acc.: 53.12%] [G loss: 0.814805]\n",
      "263 [D loss: 0.717414, acc.: 56.25%] [G loss: 0.897553]\n",
      "264 [D loss: 0.715673, acc.: 50.00%] [G loss: 0.921202]\n",
      "265 [D loss: 0.614544, acc.: 68.75%] [G loss: 0.959766]\n",
      "266 [D loss: 0.693739, acc.: 43.75%] [G loss: 0.791285]\n",
      "267 [D loss: 0.699471, acc.: 46.88%] [G loss: 0.860260]\n",
      "268 [D loss: 0.623454, acc.: 62.50%] [G loss: 0.807896]\n",
      "269 [D loss: 0.728378, acc.: 50.00%] [G loss: 0.759789]\n",
      "270 [D loss: 0.656594, acc.: 43.75%] [G loss: 0.910273]\n",
      "271 [D loss: 0.694910, acc.: 43.75%] [G loss: 0.834284]\n",
      "272 [D loss: 0.686918, acc.: 46.88%] [G loss: 0.779230]\n",
      "273 [D loss: 0.676211, acc.: 50.00%] [G loss: 0.829575]\n",
      "274 [D loss: 0.615006, acc.: 50.00%] [G loss: 0.891079]\n",
      "275 [D loss: 0.831688, acc.: 28.12%] [G loss: 0.615661]\n",
      "276 [D loss: 0.723024, acc.: 40.62%] [G loss: 0.736681]\n",
      "277 [D loss: 0.644730, acc.: 62.50%] [G loss: 0.820757]\n",
      "278 [D loss: 0.778614, acc.: 37.50%] [G loss: 0.695923]\n",
      "279 [D loss: 0.663138, acc.: 50.00%] [G loss: 0.787466]\n",
      "280 [D loss: 0.772668, acc.: 37.50%] [G loss: 0.747733]\n",
      "281 [D loss: 0.742281, acc.: 46.88%] [G loss: 0.690417]\n",
      "282 [D loss: 0.625154, acc.: 50.00%] [G loss: 0.771362]\n",
      "283 [D loss: 0.782420, acc.: 31.25%] [G loss: 0.638195]\n",
      "284 [D loss: 0.642353, acc.: 56.25%] [G loss: 0.690274]\n",
      "285 [D loss: 0.676064, acc.: 53.12%] [G loss: 0.702716]\n",
      "286 [D loss: 0.657806, acc.: 43.75%] [G loss: 0.765100]\n",
      "287 [D loss: 0.724318, acc.: 40.62%] [G loss: 0.741409]\n",
      "288 [D loss: 0.651647, acc.: 53.12%] [G loss: 0.757571]\n",
      "289 [D loss: 0.653945, acc.: 50.00%] [G loss: 0.761287]\n",
      "290 [D loss: 0.678253, acc.: 53.12%] [G loss: 0.879129]\n",
      "291 [D loss: 0.755111, acc.: 40.62%] [G loss: 0.669770]\n",
      "292 [D loss: 0.687081, acc.: 46.88%] [G loss: 0.654747]\n",
      "293 [D loss: 0.689550, acc.: 43.75%] [G loss: 0.728485]\n",
      "294 [D loss: 0.725241, acc.: 40.62%] [G loss: 0.696474]\n",
      "295 [D loss: 0.641417, acc.: 59.38%] [G loss: 0.736836]\n",
      "296 [D loss: 0.740147, acc.: 37.50%] [G loss: 0.673069]\n",
      "297 [D loss: 0.702962, acc.: 43.75%] [G loss: 0.742660]\n",
      "298 [D loss: 0.685391, acc.: 50.00%] [G loss: 0.786202]\n",
      "299 [D loss: 0.641228, acc.: 50.00%] [G loss: 0.750133]\n",
      "300 [D loss: 0.769897, acc.: 28.12%] [G loss: 0.645971]\n",
      "301 [D loss: 0.648573, acc.: 43.75%] [G loss: 0.648381]\n",
      "302 [D loss: 0.729961, acc.: 40.62%] [G loss: 0.711413]\n",
      "303 [D loss: 0.690672, acc.: 37.50%] [G loss: 0.688575]\n",
      "304 [D loss: 0.739803, acc.: 40.62%] [G loss: 0.710312]\n",
      "305 [D loss: 0.660787, acc.: 50.00%] [G loss: 0.673683]\n",
      "306 [D loss: 0.695240, acc.: 43.75%] [G loss: 0.657371]\n",
      "307 [D loss: 0.683919, acc.: 40.62%] [G loss: 0.654857]\n",
      "308 [D loss: 0.678400, acc.: 46.88%] [G loss: 0.677859]\n",
      "309 [D loss: 0.746084, acc.: 40.62%] [G loss: 0.640170]\n",
      "310 [D loss: 0.705581, acc.: 43.75%] [G loss: 0.660497]\n",
      "311 [D loss: 0.659627, acc.: 40.62%] [G loss: 0.676491]\n",
      "312 [D loss: 0.676047, acc.: 53.12%] [G loss: 0.722624]\n",
      "313 [D loss: 0.747895, acc.: 31.25%] [G loss: 0.674191]\n",
      "314 [D loss: 0.705706, acc.: 31.25%] [G loss: 0.668263]\n",
      "315 [D loss: 0.706700, acc.: 43.75%] [G loss: 0.657323]\n",
      "316 [D loss: 0.692182, acc.: 46.88%] [G loss: 0.683153]\n",
      "317 [D loss: 0.655444, acc.: 50.00%] [G loss: 0.681324]\n",
      "318 [D loss: 0.697959, acc.: 43.75%] [G loss: 0.701710]\n",
      "319 [D loss: 0.694847, acc.: 40.62%] [G loss: 0.670354]\n",
      "320 [D loss: 0.657913, acc.: 53.12%] [G loss: 0.698573]\n",
      "321 [D loss: 0.672746, acc.: 43.75%] [G loss: 0.700539]\n",
      "322 [D loss: 0.647150, acc.: 50.00%] [G loss: 0.703782]\n",
      "323 [D loss: 0.676429, acc.: 43.75%] [G loss: 0.685486]\n",
      "324 [D loss: 0.709758, acc.: 43.75%] [G loss: 0.653389]\n",
      "325 [D loss: 0.688928, acc.: 40.62%] [G loss: 0.694206]\n",
      "326 [D loss: 0.700801, acc.: 46.88%] [G loss: 0.676916]\n",
      "327 [D loss: 0.692011, acc.: 46.88%] [G loss: 0.640781]\n",
      "328 [D loss: 0.685743, acc.: 37.50%] [G loss: 0.659958]\n",
      "329 [D loss: 0.662972, acc.: 46.88%] [G loss: 0.656762]\n",
      "330 [D loss: 0.660695, acc.: 46.88%] [G loss: 0.663304]\n",
      "331 [D loss: 0.651027, acc.: 50.00%] [G loss: 0.685720]\n",
      "332 [D loss: 0.706057, acc.: 31.25%] [G loss: 0.675912]\n",
      "333 [D loss: 0.669406, acc.: 50.00%] [G loss: 0.639323]\n",
      "334 [D loss: 0.669417, acc.: 43.75%] [G loss: 0.660687]\n",
      "335 [D loss: 0.683438, acc.: 43.75%] [G loss: 0.654979]\n",
      "336 [D loss: 0.656458, acc.: 46.88%] [G loss: 0.665987]\n",
      "337 [D loss: 0.660884, acc.: 50.00%] [G loss: 0.691464]\n",
      "338 [D loss: 0.691386, acc.: 43.75%] [G loss: 0.670467]\n",
      "339 [D loss: 0.670223, acc.: 53.12%] [G loss: 0.646980]\n",
      "340 [D loss: 0.645104, acc.: 56.25%] [G loss: 0.672639]\n",
      "341 [D loss: 0.708566, acc.: 40.62%] [G loss: 0.651567]\n",
      "342 [D loss: 0.679052, acc.: 50.00%] [G loss: 0.676875]\n",
      "343 [D loss: 0.668819, acc.: 46.88%] [G loss: 0.652674]\n",
      "344 [D loss: 0.684334, acc.: 46.88%] [G loss: 0.686646]\n",
      "345 [D loss: 0.645201, acc.: 53.12%] [G loss: 0.689707]\n",
      "346 [D loss: 0.674721, acc.: 46.88%] [G loss: 0.681554]\n",
      "347 [D loss: 0.703362, acc.: 43.75%] [G loss: 0.638751]\n",
      "348 [D loss: 0.658807, acc.: 46.88%] [G loss: 0.654544]\n",
      "349 [D loss: 0.651981, acc.: 50.00%] [G loss: 0.695845]\n",
      "350 [D loss: 0.676888, acc.: 50.00%] [G loss: 0.687893]\n",
      "351 [D loss: 0.675508, acc.: 46.88%] [G loss: 0.709802]\n",
      "352 [D loss: 0.652478, acc.: 46.88%] [G loss: 0.737842]\n",
      "353 [D loss: 0.706296, acc.: 46.88%] [G loss: 0.642195]\n",
      "354 [D loss: 0.678872, acc.: 46.88%] [G loss: 0.618588]\n",
      "355 [D loss: 0.652937, acc.: 50.00%] [G loss: 0.618957]\n",
      "356 [D loss: 0.690167, acc.: 50.00%] [G loss: 0.622498]\n",
      "357 [D loss: 0.684267, acc.: 50.00%] [G loss: 0.638617]\n",
      "358 [D loss: 0.695647, acc.: 50.00%] [G loss: 0.673218]\n",
      "359 [D loss: 0.676866, acc.: 50.00%] [G loss: 0.655886]\n",
      "360 [D loss: 0.697787, acc.: 50.00%] [G loss: 0.665269]\n",
      "361 [D loss: 0.655681, acc.: 46.88%] [G loss: 0.667870]\n",
      "362 [D loss: 0.673802, acc.: 50.00%] [G loss: 0.670694]\n",
      "363 [D loss: 0.673993, acc.: 50.00%] [G loss: 0.696893]\n",
      "364 [D loss: 0.680051, acc.: 46.88%] [G loss: 0.696961]\n",
      "365 [D loss: 0.687486, acc.: 46.88%] [G loss: 0.687730]\n",
      "366 [D loss: 0.666439, acc.: 50.00%] [G loss: 0.703753]\n",
      "367 [D loss: 0.674859, acc.: 50.00%] [G loss: 0.673007]\n",
      "368 [D loss: 0.693518, acc.: 50.00%] [G loss: 0.670950]\n",
      "369 [D loss: 0.691078, acc.: 46.88%] [G loss: 0.715058]\n",
      "370 [D loss: 0.687928, acc.: 43.75%] [G loss: 0.652717]\n",
      "371 [D loss: 0.680010, acc.: 50.00%] [G loss: 0.626498]\n",
      "372 [D loss: 0.706893, acc.: 50.00%] [G loss: 0.665938]\n",
      "373 [D loss: 0.686444, acc.: 46.88%] [G loss: 0.691577]\n",
      "374 [D loss: 0.645465, acc.: 59.38%] [G loss: 0.703444]\n",
      "375 [D loss: 0.681641, acc.: 46.88%] [G loss: 0.673076]\n",
      "376 [D loss: 0.717600, acc.: 46.88%] [G loss: 0.671354]\n",
      "377 [D loss: 0.686192, acc.: 50.00%] [G loss: 0.717145]\n",
      "378 [D loss: 0.674820, acc.: 53.12%] [G loss: 0.713797]\n",
      "379 [D loss: 0.688757, acc.: 43.75%] [G loss: 0.686996]\n",
      "380 [D loss: 0.670838, acc.: 53.12%] [G loss: 0.694756]\n",
      "381 [D loss: 0.686677, acc.: 50.00%] [G loss: 0.648029]\n",
      "382 [D loss: 0.677324, acc.: 53.12%] [G loss: 0.679959]\n",
      "383 [D loss: 0.674150, acc.: 56.25%] [G loss: 0.739515]\n",
      "384 [D loss: 0.684868, acc.: 40.62%] [G loss: 0.696426]\n",
      "385 [D loss: 0.670714, acc.: 53.12%] [G loss: 0.721450]\n",
      "386 [D loss: 0.735015, acc.: 37.50%] [G loss: 0.633733]\n",
      "387 [D loss: 0.684521, acc.: 50.00%] [G loss: 0.624662]\n",
      "388 [D loss: 0.694604, acc.: 50.00%] [G loss: 0.625382]\n",
      "389 [D loss: 0.661643, acc.: 50.00%] [G loss: 0.647386]\n",
      "390 [D loss: 0.670566, acc.: 46.88%] [G loss: 0.660251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391 [D loss: 0.613777, acc.: 62.50%] [G loss: 0.706074]\n",
      "392 [D loss: 0.639859, acc.: 65.62%] [G loss: 0.722366]\n",
      "393 [D loss: 0.696026, acc.: 46.88%] [G loss: 0.671283]\n",
      "394 [D loss: 0.659259, acc.: 56.25%] [G loss: 0.660230]\n",
      "395 [D loss: 0.703017, acc.: 46.88%] [G loss: 0.661676]\n",
      "396 [D loss: 0.663531, acc.: 46.88%] [G loss: 0.674665]\n",
      "397 [D loss: 0.677893, acc.: 50.00%] [G loss: 0.667509]\n",
      "398 [D loss: 0.668882, acc.: 50.00%] [G loss: 0.653827]\n",
      "399 [D loss: 0.659246, acc.: 50.00%] [G loss: 0.664419]\n",
      "400 [D loss: 0.685910, acc.: 46.88%] [G loss: 0.684111]\n",
      "401 [D loss: 0.669259, acc.: 46.88%] [G loss: 0.652101]\n",
      "402 [D loss: 0.647265, acc.: 53.12%] [G loss: 0.639825]\n",
      "403 [D loss: 0.651917, acc.: 56.25%] [G loss: 0.653257]\n",
      "404 [D loss: 0.650822, acc.: 59.38%] [G loss: 0.680625]\n",
      "405 [D loss: 0.690994, acc.: 46.88%] [G loss: 0.663951]\n",
      "406 [D loss: 0.680166, acc.: 46.88%] [G loss: 0.650104]\n",
      "407 [D loss: 0.677611, acc.: 50.00%] [G loss: 0.650855]\n",
      "408 [D loss: 0.688384, acc.: 50.00%] [G loss: 0.628851]\n",
      "409 [D loss: 0.638255, acc.: 53.12%] [G loss: 0.660522]\n",
      "410 [D loss: 0.671578, acc.: 46.88%] [G loss: 0.659283]\n",
      "411 [D loss: 0.661859, acc.: 46.88%] [G loss: 0.669645]\n",
      "412 [D loss: 0.632245, acc.: 59.38%] [G loss: 0.668655]\n",
      "413 [D loss: 0.635199, acc.: 59.38%] [G loss: 0.670992]\n",
      "414 [D loss: 0.668306, acc.: 50.00%] [G loss: 0.677914]\n",
      "415 [D loss: 0.630854, acc.: 50.00%] [G loss: 0.671415]\n",
      "416 [D loss: 0.630470, acc.: 53.12%] [G loss: 0.672646]\n",
      "417 [D loss: 0.627315, acc.: 59.38%] [G loss: 0.679268]\n",
      "418 [D loss: 0.627232, acc.: 62.50%] [G loss: 0.698043]\n",
      "419 [D loss: 0.676622, acc.: 50.00%] [G loss: 0.706466]\n",
      "420 [D loss: 0.616414, acc.: 59.38%] [G loss: 0.726380]\n",
      "421 [D loss: 0.656428, acc.: 43.75%] [G loss: 0.705598]\n",
      "422 [D loss: 0.660571, acc.: 40.62%] [G loss: 0.691701]\n",
      "423 [D loss: 0.655992, acc.: 50.00%] [G loss: 0.680993]\n",
      "424 [D loss: 0.664350, acc.: 43.75%] [G loss: 0.668427]\n",
      "425 [D loss: 0.655979, acc.: 50.00%] [G loss: 0.667363]\n",
      "426 [D loss: 0.674826, acc.: 46.88%] [G loss: 0.693268]\n",
      "427 [D loss: 0.648259, acc.: 56.25%] [G loss: 0.706974]\n",
      "428 [D loss: 0.626844, acc.: 68.75%] [G loss: 0.719680]\n",
      "429 [D loss: 0.668088, acc.: 50.00%] [G loss: 0.687271]\n",
      "430 [D loss: 0.652920, acc.: 50.00%] [G loss: 0.669935]\n",
      "431 [D loss: 0.659210, acc.: 46.88%] [G loss: 0.669810]\n",
      "432 [D loss: 0.660800, acc.: 50.00%] [G loss: 0.677247]\n",
      "433 [D loss: 0.649897, acc.: 50.00%] [G loss: 0.684983]\n",
      "434 [D loss: 0.651482, acc.: 46.88%] [G loss: 0.696253]\n",
      "435 [D loss: 0.638689, acc.: 53.12%] [G loss: 0.693963]\n",
      "436 [D loss: 0.628421, acc.: 46.88%] [G loss: 0.696733]\n",
      "437 [D loss: 0.651114, acc.: 46.88%] [G loss: 0.675058]\n",
      "438 [D loss: 0.645785, acc.: 53.12%] [G loss: 0.689747]\n",
      "439 [D loss: 0.617808, acc.: 59.38%] [G loss: 0.681759]\n",
      "440 [D loss: 0.663235, acc.: 46.88%] [G loss: 0.703790]\n",
      "441 [D loss: 0.647358, acc.: 46.88%] [G loss: 0.720821]\n",
      "442 [D loss: 0.617210, acc.: 65.62%] [G loss: 0.731918]\n",
      "443 [D loss: 0.648759, acc.: 53.12%] [G loss: 0.716514]\n",
      "444 [D loss: 0.643124, acc.: 50.00%] [G loss: 0.700196]\n",
      "445 [D loss: 0.662876, acc.: 46.88%] [G loss: 0.693237]\n",
      "446 [D loss: 0.628786, acc.: 62.50%] [G loss: 0.706106]\n",
      "447 [D loss: 0.628752, acc.: 53.12%] [G loss: 0.696248]\n",
      "448 [D loss: 0.646454, acc.: 56.25%] [G loss: 0.715142]\n",
      "449 [D loss: 0.670556, acc.: 50.00%] [G loss: 0.712743]\n",
      "450 [D loss: 0.651007, acc.: 53.12%] [G loss: 0.714890]\n",
      "451 [D loss: 0.644933, acc.: 53.12%] [G loss: 0.706459]\n",
      "452 [D loss: 0.686748, acc.: 43.75%] [G loss: 0.742681]\n",
      "453 [D loss: 0.683272, acc.: 43.75%] [G loss: 0.716031]\n",
      "454 [D loss: 0.703490, acc.: 50.00%] [G loss: 0.690885]\n",
      "455 [D loss: 0.638081, acc.: 56.25%] [G loss: 0.684841]\n",
      "456 [D loss: 0.679070, acc.: 46.88%] [G loss: 0.700126]\n",
      "457 [D loss: 0.645314, acc.: 50.00%] [G loss: 0.703176]\n",
      "458 [D loss: 0.643137, acc.: 68.75%] [G loss: 0.691650]\n",
      "459 [D loss: 0.656377, acc.: 50.00%] [G loss: 0.710305]\n",
      "460 [D loss: 0.648405, acc.: 46.88%] [G loss: 0.702499]\n",
      "461 [D loss: 0.641880, acc.: 59.38%] [G loss: 0.717128]\n",
      "462 [D loss: 0.636650, acc.: 62.50%] [G loss: 0.728850]\n",
      "463 [D loss: 0.641080, acc.: 75.00%] [G loss: 0.733626]\n",
      "464 [D loss: 0.637279, acc.: 62.50%] [G loss: 0.734717]\n",
      "465 [D loss: 0.651461, acc.: 53.12%] [G loss: 0.746423]\n",
      "466 [D loss: 0.651465, acc.: 59.38%] [G loss: 0.759452]\n",
      "467 [D loss: 0.638074, acc.: 56.25%] [G loss: 0.782378]\n",
      "468 [D loss: 0.675378, acc.: 43.75%] [G loss: 0.749137]\n",
      "469 [D loss: 0.653492, acc.: 50.00%] [G loss: 0.722500]\n",
      "470 [D loss: 0.700207, acc.: 50.00%] [G loss: 0.709734]\n",
      "471 [D loss: 0.643441, acc.: 50.00%] [G loss: 0.712863]\n",
      "472 [D loss: 0.642269, acc.: 59.38%] [G loss: 0.719569]\n",
      "473 [D loss: 0.659257, acc.: 46.88%] [G loss: 0.707545]\n",
      "474 [D loss: 0.644913, acc.: 59.38%] [G loss: 0.706911]\n",
      "475 [D loss: 0.654339, acc.: 53.12%] [G loss: 0.709292]\n",
      "476 [D loss: 0.644295, acc.: 59.38%] [G loss: 0.705364]\n",
      "477 [D loss: 0.674792, acc.: 50.00%] [G loss: 0.696481]\n",
      "478 [D loss: 0.641970, acc.: 50.00%] [G loss: 0.690036]\n",
      "479 [D loss: 0.695226, acc.: 46.88%] [G loss: 0.684240]\n",
      "480 [D loss: 0.663147, acc.: 50.00%] [G loss: 0.691466]\n",
      "481 [D loss: 0.661481, acc.: 46.88%] [G loss: 0.689535]\n",
      "482 [D loss: 0.676307, acc.: 50.00%] [G loss: 0.722186]\n",
      "483 [D loss: 0.680785, acc.: 53.12%] [G loss: 0.716910]\n",
      "484 [D loss: 0.665574, acc.: 62.50%] [G loss: 0.739354]\n",
      "485 [D loss: 0.676399, acc.: 43.75%] [G loss: 0.717172]\n",
      "486 [D loss: 0.678877, acc.: 53.12%] [G loss: 0.710922]\n",
      "487 [D loss: 0.671894, acc.: 59.38%] [G loss: 0.709044]\n",
      "488 [D loss: 0.687469, acc.: 50.00%] [G loss: 0.696978]\n",
      "489 [D loss: 0.626397, acc.: 68.75%] [G loss: 0.721310]\n",
      "490 [D loss: 0.643836, acc.: 53.12%] [G loss: 0.711468]\n",
      "491 [D loss: 0.687122, acc.: 59.38%] [G loss: 0.677801]\n",
      "492 [D loss: 0.668083, acc.: 53.12%] [G loss: 0.669459]\n",
      "493 [D loss: 0.661462, acc.: 46.88%] [G loss: 0.647557]\n",
      "494 [D loss: 0.663968, acc.: 59.38%] [G loss: 0.649594]\n",
      "495 [D loss: 0.675731, acc.: 62.50%] [G loss: 0.665234]\n",
      "496 [D loss: 0.655894, acc.: 46.88%] [G loss: 0.690451]\n",
      "497 [D loss: 0.655960, acc.: 46.88%] [G loss: 0.699709]\n",
      "498 [D loss: 0.637784, acc.: 59.38%] [G loss: 0.691340]\n",
      "499 [D loss: 0.635658, acc.: 56.25%] [G loss: 0.687529]\n",
      "500 [D loss: 0.640879, acc.: 59.38%] [G loss: 0.688639]\n",
      "501 [D loss: 0.642828, acc.: 62.50%] [G loss: 0.693660]\n",
      "502 [D loss: 0.655100, acc.: 65.62%] [G loss: 0.701904]\n",
      "503 [D loss: 0.641649, acc.: 68.75%] [G loss: 0.717757]\n",
      "504 [D loss: 0.674549, acc.: 68.75%] [G loss: 0.707859]\n",
      "505 [D loss: 0.668780, acc.: 62.50%] [G loss: 0.707276]\n",
      "506 [D loss: 0.623817, acc.: 65.62%] [G loss: 0.733632]\n",
      "507 [D loss: 0.650113, acc.: 56.25%] [G loss: 0.693570]\n",
      "508 [D loss: 0.642531, acc.: 68.75%] [G loss: 0.664702]\n",
      "509 [D loss: 0.641843, acc.: 53.12%] [G loss: 0.690307]\n",
      "510 [D loss: 0.629222, acc.: 78.12%] [G loss: 0.712258]\n",
      "511 [D loss: 0.626021, acc.: 71.88%] [G loss: 0.716907]\n",
      "512 [D loss: 0.641264, acc.: 75.00%] [G loss: 0.696002]\n",
      "513 [D loss: 0.605789, acc.: 68.75%] [G loss: 0.711288]\n",
      "514 [D loss: 0.647180, acc.: 50.00%] [G loss: 0.708335]\n",
      "515 [D loss: 0.603789, acc.: 62.50%] [G loss: 0.719699]\n",
      "516 [D loss: 0.676299, acc.: 65.62%] [G loss: 0.724250]\n",
      "517 [D loss: 0.696327, acc.: 59.38%] [G loss: 0.737977]\n",
      "518 [D loss: 0.647781, acc.: 62.50%] [G loss: 0.771708]\n",
      "519 [D loss: 0.675587, acc.: 56.25%] [G loss: 0.732763]\n",
      "520 [D loss: 0.644393, acc.: 56.25%] [G loss: 0.730158]\n",
      "521 [D loss: 0.636062, acc.: 65.62%] [G loss: 0.737727]\n",
      "522 [D loss: 0.643151, acc.: 53.12%] [G loss: 0.724510]\n",
      "523 [D loss: 0.628788, acc.: 62.50%] [G loss: 0.730233]\n",
      "524 [D loss: 0.641475, acc.: 62.50%] [G loss: 0.713238]\n",
      "525 [D loss: 0.663854, acc.: 50.00%] [G loss: 0.704338]\n",
      "526 [D loss: 0.669437, acc.: 59.38%] [G loss: 0.734153]\n",
      "527 [D loss: 0.615080, acc.: 65.62%] [G loss: 0.742413]\n",
      "528 [D loss: 0.665795, acc.: 59.38%] [G loss: 0.723927]\n",
      "529 [D loss: 0.647627, acc.: 62.50%] [G loss: 0.727063]\n",
      "530 [D loss: 0.623703, acc.: 68.75%] [G loss: 0.723338]\n",
      "531 [D loss: 0.627349, acc.: 68.75%] [G loss: 0.743652]\n",
      "532 [D loss: 0.668596, acc.: 65.62%] [G loss: 0.694914]\n",
      "533 [D loss: 0.637867, acc.: 62.50%] [G loss: 0.690650]\n",
      "534 [D loss: 0.698053, acc.: 50.00%] [G loss: 0.672027]\n",
      "535 [D loss: 0.655366, acc.: 46.88%] [G loss: 0.699949]\n",
      "536 [D loss: 0.641849, acc.: 62.50%] [G loss: 0.737181]\n",
      "537 [D loss: 0.667953, acc.: 43.75%] [G loss: 0.771151]\n",
      "538 [D loss: 0.627237, acc.: 75.00%] [G loss: 0.769418]\n",
      "539 [D loss: 0.621114, acc.: 75.00%] [G loss: 0.736171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540 [D loss: 0.705228, acc.: 56.25%] [G loss: 0.761503]\n",
      "541 [D loss: 0.669871, acc.: 53.12%] [G loss: 0.774188]\n",
      "542 [D loss: 0.631152, acc.: 75.00%] [G loss: 0.734368]\n",
      "543 [D loss: 0.678093, acc.: 53.12%] [G loss: 0.714154]\n",
      "544 [D loss: 0.675807, acc.: 46.88%] [G loss: 0.678501]\n",
      "545 [D loss: 0.650694, acc.: 59.38%] [G loss: 0.689516]\n",
      "546 [D loss: 0.674183, acc.: 46.88%] [G loss: 0.698618]\n",
      "547 [D loss: 0.616891, acc.: 62.50%] [G loss: 0.713340]\n",
      "548 [D loss: 0.637919, acc.: 68.75%] [G loss: 0.717936]\n",
      "549 [D loss: 0.634680, acc.: 59.38%] [G loss: 0.749140]\n",
      "550 [D loss: 0.654767, acc.: 56.25%] [G loss: 0.752324]\n",
      "551 [D loss: 0.660820, acc.: 59.38%] [G loss: 0.739130]\n",
      "552 [D loss: 0.635626, acc.: 65.62%] [G loss: 0.757651]\n",
      "553 [D loss: 0.662622, acc.: 65.62%] [G loss: 0.736794]\n",
      "554 [D loss: 0.676238, acc.: 53.12%] [G loss: 0.708562]\n",
      "555 [D loss: 0.655115, acc.: 65.62%] [G loss: 0.694236]\n",
      "556 [D loss: 0.662711, acc.: 59.38%] [G loss: 0.673975]\n",
      "557 [D loss: 0.667223, acc.: 56.25%] [G loss: 0.677097]\n",
      "558 [D loss: 0.617961, acc.: 75.00%] [G loss: 0.682718]\n",
      "559 [D loss: 0.664649, acc.: 68.75%] [G loss: 0.747164]\n",
      "560 [D loss: 0.643655, acc.: 75.00%] [G loss: 0.759661]\n",
      "561 [D loss: 0.736735, acc.: 37.50%] [G loss: 0.766604]\n",
      "562 [D loss: 0.676435, acc.: 62.50%] [G loss: 0.760547]\n",
      "563 [D loss: 0.658081, acc.: 65.62%] [G loss: 0.740933]\n",
      "564 [D loss: 0.668240, acc.: 53.12%] [G loss: 0.738026]\n",
      "565 [D loss: 0.695235, acc.: 53.12%] [G loss: 0.749496]\n",
      "566 [D loss: 0.689433, acc.: 46.88%] [G loss: 0.757661]\n",
      "567 [D loss: 0.705803, acc.: 46.88%] [G loss: 0.688576]\n",
      "568 [D loss: 0.681760, acc.: 53.12%] [G loss: 0.662210]\n",
      "569 [D loss: 0.678220, acc.: 59.38%] [G loss: 0.687344]\n",
      "570 [D loss: 0.664950, acc.: 59.38%] [G loss: 0.709302]\n",
      "571 [D loss: 0.689832, acc.: 43.75%] [G loss: 0.710505]\n",
      "572 [D loss: 0.699394, acc.: 46.88%] [G loss: 0.731376]\n",
      "573 [D loss: 0.640959, acc.: 71.88%] [G loss: 0.729836]\n",
      "574 [D loss: 0.673565, acc.: 53.12%] [G loss: 0.703830]\n",
      "575 [D loss: 0.680685, acc.: 50.00%] [G loss: 0.687520]\n",
      "576 [D loss: 0.660050, acc.: 62.50%] [G loss: 0.679058]\n",
      "577 [D loss: 0.690283, acc.: 59.38%] [G loss: 0.697700]\n",
      "578 [D loss: 0.661838, acc.: 53.12%] [G loss: 0.690423]\n",
      "579 [D loss: 0.659266, acc.: 50.00%] [G loss: 0.709543]\n",
      "580 [D loss: 0.643592, acc.: 68.75%] [G loss: 0.697143]\n",
      "581 [D loss: 0.633431, acc.: 62.50%] [G loss: 0.687601]\n",
      "582 [D loss: 0.674475, acc.: 53.12%] [G loss: 0.680514]\n",
      "583 [D loss: 0.658138, acc.: 59.38%] [G loss: 0.662951]\n",
      "584 [D loss: 0.653685, acc.: 59.38%] [G loss: 0.691542]\n",
      "585 [D loss: 0.682522, acc.: 56.25%] [G loss: 0.725986]\n",
      "586 [D loss: 0.667532, acc.: 62.50%] [G loss: 0.732288]\n",
      "587 [D loss: 0.655083, acc.: 59.38%] [G loss: 0.729266]\n",
      "588 [D loss: 0.649855, acc.: 65.62%] [G loss: 0.686799]\n",
      "589 [D loss: 0.657754, acc.: 56.25%] [G loss: 0.685522]\n",
      "590 [D loss: 0.662540, acc.: 59.38%] [G loss: 0.663453]\n",
      "591 [D loss: 0.639127, acc.: 65.62%] [G loss: 0.703141]\n",
      "592 [D loss: 0.608011, acc.: 71.88%] [G loss: 0.712365]\n",
      "593 [D loss: 0.663944, acc.: 56.25%] [G loss: 0.703911]\n",
      "594 [D loss: 0.673061, acc.: 56.25%] [G loss: 0.734715]\n",
      "595 [D loss: 0.643265, acc.: 62.50%] [G loss: 0.727439]\n",
      "596 [D loss: 0.658377, acc.: 62.50%] [G loss: 0.694725]\n",
      "597 [D loss: 0.623289, acc.: 59.38%] [G loss: 0.719726]\n",
      "598 [D loss: 0.688483, acc.: 59.38%] [G loss: 0.738826]\n",
      "599 [D loss: 0.671784, acc.: 59.38%] [G loss: 0.771603]\n",
      "600 [D loss: 0.656947, acc.: 68.75%] [G loss: 0.742577]\n",
      "601 [D loss: 0.663237, acc.: 71.88%] [G loss: 0.722401]\n",
      "602 [D loss: 0.626877, acc.: 62.50%] [G loss: 0.708947]\n",
      "603 [D loss: 0.662273, acc.: 59.38%] [G loss: 0.728502]\n",
      "604 [D loss: 0.622076, acc.: 71.88%] [G loss: 0.728996]\n",
      "605 [D loss: 0.650749, acc.: 59.38%] [G loss: 0.730040]\n",
      "606 [D loss: 0.666796, acc.: 65.62%] [G loss: 0.751278]\n",
      "607 [D loss: 0.669005, acc.: 56.25%] [G loss: 0.733946]\n",
      "608 [D loss: 0.651351, acc.: 59.38%] [G loss: 0.731043]\n",
      "609 [D loss: 0.589447, acc.: 84.38%] [G loss: 0.704554]\n",
      "610 [D loss: 0.622406, acc.: 65.62%] [G loss: 0.712008]\n",
      "611 [D loss: 0.661887, acc.: 59.38%] [G loss: 0.733813]\n",
      "612 [D loss: 0.658687, acc.: 59.38%] [G loss: 0.760862]\n",
      "613 [D loss: 0.625599, acc.: 75.00%] [G loss: 0.778253]\n",
      "614 [D loss: 0.659498, acc.: 59.38%] [G loss: 0.753302]\n",
      "615 [D loss: 0.655064, acc.: 71.88%] [G loss: 0.758747]\n",
      "616 [D loss: 0.611974, acc.: 65.62%] [G loss: 0.720928]\n",
      "617 [D loss: 0.668932, acc.: 50.00%] [G loss: 0.734972]\n",
      "618 [D loss: 0.627308, acc.: 65.62%] [G loss: 0.726937]\n",
      "619 [D loss: 0.645888, acc.: 46.88%] [G loss: 0.746548]\n",
      "620 [D loss: 0.651554, acc.: 50.00%] [G loss: 0.751756]\n",
      "621 [D loss: 0.648788, acc.: 59.38%] [G loss: 0.727964]\n",
      "622 [D loss: 0.634027, acc.: 65.62%] [G loss: 0.748499]\n",
      "623 [D loss: 0.633725, acc.: 68.75%] [G loss: 0.751860]\n",
      "624 [D loss: 0.624603, acc.: 71.88%] [G loss: 0.748953]\n",
      "625 [D loss: 0.669249, acc.: 56.25%] [G loss: 0.712626]\n",
      "626 [D loss: 0.635058, acc.: 71.88%] [G loss: 0.722001]\n",
      "627 [D loss: 0.612007, acc.: 65.62%] [G loss: 0.729385]\n",
      "628 [D loss: 0.632213, acc.: 65.62%] [G loss: 0.720709]\n",
      "629 [D loss: 0.672366, acc.: 56.25%] [G loss: 0.726279]\n",
      "630 [D loss: 0.616494, acc.: 71.88%] [G loss: 0.730977]\n",
      "631 [D loss: 0.625522, acc.: 62.50%] [G loss: 0.732957]\n",
      "632 [D loss: 0.629618, acc.: 65.62%] [G loss: 0.790419]\n",
      "633 [D loss: 0.634179, acc.: 68.75%] [G loss: 0.777533]\n",
      "634 [D loss: 0.589648, acc.: 78.12%] [G loss: 0.771695]\n",
      "635 [D loss: 0.618068, acc.: 75.00%] [G loss: 0.756449]\n",
      "636 [D loss: 0.613002, acc.: 68.75%] [G loss: 0.757107]\n",
      "637 [D loss: 0.662025, acc.: 56.25%] [G loss: 0.723378]\n",
      "638 [D loss: 0.629405, acc.: 62.50%] [G loss: 0.743072]\n",
      "639 [D loss: 0.656339, acc.: 53.12%] [G loss: 0.754861]\n",
      "640 [D loss: 0.587101, acc.: 78.12%] [G loss: 0.778795]\n",
      "641 [D loss: 0.654292, acc.: 62.50%] [G loss: 0.789240]\n",
      "642 [D loss: 0.625914, acc.: 68.75%] [G loss: 0.765134]\n",
      "643 [D loss: 0.659491, acc.: 50.00%] [G loss: 0.726930]\n",
      "644 [D loss: 0.592501, acc.: 71.88%] [G loss: 0.732427]\n",
      "645 [D loss: 0.607036, acc.: 59.38%] [G loss: 0.776682]\n",
      "646 [D loss: 0.620704, acc.: 59.38%] [G loss: 0.806112]\n",
      "647 [D loss: 0.670969, acc.: 50.00%] [G loss: 0.754049]\n",
      "648 [D loss: 0.668211, acc.: 59.38%] [G loss: 0.745168]\n",
      "649 [D loss: 0.565171, acc.: 81.25%] [G loss: 0.749939]\n",
      "650 [D loss: 0.678494, acc.: 56.25%] [G loss: 0.756572]\n",
      "651 [D loss: 0.712387, acc.: 56.25%] [G loss: 0.756715]\n",
      "652 [D loss: 0.644753, acc.: 62.50%] [G loss: 0.796714]\n",
      "653 [D loss: 0.642507, acc.: 59.38%] [G loss: 0.756498]\n",
      "654 [D loss: 0.667446, acc.: 59.38%] [G loss: 0.726525]\n",
      "655 [D loss: 0.667880, acc.: 50.00%] [G loss: 0.772271]\n",
      "656 [D loss: 0.659671, acc.: 50.00%] [G loss: 0.827989]\n",
      "657 [D loss: 0.677805, acc.: 50.00%] [G loss: 0.739528]\n",
      "658 [D loss: 0.689140, acc.: 43.75%] [G loss: 0.730058]\n",
      "659 [D loss: 0.676095, acc.: 46.88%] [G loss: 0.742250]\n",
      "660 [D loss: 0.706265, acc.: 46.88%] [G loss: 0.746740]\n",
      "661 [D loss: 0.694674, acc.: 53.12%] [G loss: 0.741251]\n",
      "662 [D loss: 0.632367, acc.: 59.38%] [G loss: 0.701798]\n",
      "663 [D loss: 0.691746, acc.: 43.75%] [G loss: 0.695938]\n",
      "664 [D loss: 0.654164, acc.: 68.75%] [G loss: 0.722916]\n",
      "665 [D loss: 0.664119, acc.: 65.62%] [G loss: 0.736744]\n",
      "666 [D loss: 0.645868, acc.: 56.25%] [G loss: 0.709982]\n",
      "667 [D loss: 0.626893, acc.: 68.75%] [G loss: 0.725100]\n",
      "668 [D loss: 0.656238, acc.: 59.38%] [G loss: 0.793448]\n",
      "669 [D loss: 0.663791, acc.: 59.38%] [G loss: 0.816376]\n",
      "670 [D loss: 0.680231, acc.: 53.12%] [G loss: 0.755099]\n",
      "671 [D loss: 0.643534, acc.: 75.00%] [G loss: 0.740019]\n",
      "672 [D loss: 0.701894, acc.: 50.00%] [G loss: 0.751702]\n",
      "673 [D loss: 0.668344, acc.: 53.12%] [G loss: 0.765869]\n",
      "674 [D loss: 0.728867, acc.: 43.75%] [G loss: 0.744263]\n",
      "675 [D loss: 0.702209, acc.: 53.12%] [G loss: 0.713733]\n",
      "676 [D loss: 0.685159, acc.: 50.00%] [G loss: 0.769518]\n",
      "677 [D loss: 0.654967, acc.: 53.12%] [G loss: 0.770451]\n",
      "678 [D loss: 0.668279, acc.: 53.12%] [G loss: 0.795753]\n",
      "679 [D loss: 0.633122, acc.: 75.00%] [G loss: 0.786183]\n",
      "680 [D loss: 0.617657, acc.: 68.75%] [G loss: 0.737040]\n",
      "681 [D loss: 0.666308, acc.: 62.50%] [G loss: 0.757433]\n",
      "682 [D loss: 0.661811, acc.: 56.25%] [G loss: 0.759054]\n",
      "683 [D loss: 0.644368, acc.: 65.62%] [G loss: 0.763558]\n",
      "684 [D loss: 0.617082, acc.: 65.62%] [G loss: 0.820695]\n",
      "685 [D loss: 0.616192, acc.: 65.62%] [G loss: 0.779562]\n",
      "686 [D loss: 0.661032, acc.: 59.38%] [G loss: 0.759241]\n",
      "687 [D loss: 0.634290, acc.: 65.62%] [G loss: 0.782709]\n",
      "688 [D loss: 0.648512, acc.: 56.25%] [G loss: 0.814060]\n",
      "689 [D loss: 0.649029, acc.: 62.50%] [G loss: 0.795395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690 [D loss: 0.620612, acc.: 62.50%] [G loss: 0.818941]\n",
      "691 [D loss: 0.678432, acc.: 53.12%] [G loss: 0.781900]\n",
      "692 [D loss: 0.600666, acc.: 68.75%] [G loss: 0.813287]\n",
      "693 [D loss: 0.631477, acc.: 65.62%] [G loss: 0.825718]\n",
      "694 [D loss: 0.688216, acc.: 53.12%] [G loss: 0.766312]\n",
      "695 [D loss: 0.621343, acc.: 59.38%] [G loss: 0.759763]\n",
      "696 [D loss: 0.624545, acc.: 59.38%] [G loss: 0.777318]\n",
      "697 [D loss: 0.634026, acc.: 65.62%] [G loss: 0.729448]\n",
      "698 [D loss: 0.674647, acc.: 59.38%] [G loss: 0.719397]\n",
      "699 [D loss: 0.625218, acc.: 56.25%] [G loss: 0.761912]\n",
      "700 [D loss: 0.646619, acc.: 62.50%] [G loss: 0.795921]\n",
      "701 [D loss: 0.586791, acc.: 78.12%] [G loss: 0.817861]\n",
      "702 [D loss: 0.644920, acc.: 68.75%] [G loss: 0.734843]\n",
      "703 [D loss: 0.667965, acc.: 59.38%] [G loss: 0.709831]\n",
      "704 [D loss: 0.663844, acc.: 43.75%] [G loss: 0.717304]\n",
      "705 [D loss: 0.678718, acc.: 50.00%] [G loss: 0.765432]\n",
      "706 [D loss: 0.663916, acc.: 59.38%] [G loss: 0.764145]\n",
      "707 [D loss: 0.633947, acc.: 59.38%] [G loss: 0.785707]\n",
      "708 [D loss: 0.616396, acc.: 65.62%] [G loss: 0.852696]\n",
      "709 [D loss: 0.664231, acc.: 56.25%] [G loss: 0.807637]\n",
      "710 [D loss: 0.683778, acc.: 46.88%] [G loss: 0.738081]\n",
      "711 [D loss: 0.619579, acc.: 56.25%] [G loss: 0.784170]\n",
      "712 [D loss: 0.626373, acc.: 75.00%] [G loss: 0.801087]\n",
      "713 [D loss: 0.686305, acc.: 53.12%] [G loss: 0.782083]\n",
      "714 [D loss: 0.625473, acc.: 65.62%] [G loss: 0.802993]\n",
      "715 [D loss: 0.667682, acc.: 56.25%] [G loss: 0.743439]\n",
      "716 [D loss: 0.639987, acc.: 56.25%] [G loss: 0.783471]\n",
      "717 [D loss: 0.623800, acc.: 62.50%] [G loss: 0.812174]\n",
      "718 [D loss: 0.617041, acc.: 75.00%] [G loss: 0.766070]\n",
      "719 [D loss: 0.663321, acc.: 53.12%] [G loss: 0.754750]\n",
      "720 [D loss: 0.597870, acc.: 71.88%] [G loss: 0.768745]\n",
      "721 [D loss: 0.671184, acc.: 59.38%] [G loss: 0.786570]\n",
      "722 [D loss: 0.688715, acc.: 50.00%] [G loss: 0.713288]\n",
      "723 [D loss: 0.680649, acc.: 53.12%] [G loss: 0.726906]\n",
      "724 [D loss: 0.706880, acc.: 43.75%] [G loss: 0.692708]\n",
      "725 [D loss: 0.649160, acc.: 71.88%] [G loss: 0.690567]\n",
      "726 [D loss: 0.658826, acc.: 59.38%] [G loss: 0.760320]\n",
      "727 [D loss: 0.633663, acc.: 62.50%] [G loss: 0.787125]\n",
      "728 [D loss: 0.639541, acc.: 62.50%] [G loss: 0.777766]\n",
      "729 [D loss: 0.657091, acc.: 56.25%] [G loss: 0.815984]\n",
      "730 [D loss: 0.628064, acc.: 71.88%] [G loss: 0.806009]\n",
      "731 [D loss: 0.676400, acc.: 46.88%] [G loss: 0.811079]\n",
      "732 [D loss: 0.616409, acc.: 71.88%] [G loss: 0.778268]\n",
      "733 [D loss: 0.647883, acc.: 62.50%] [G loss: 0.790483]\n",
      "734 [D loss: 0.665388, acc.: 46.88%] [G loss: 0.761473]\n",
      "735 [D loss: 0.606447, acc.: 68.75%] [G loss: 0.782302]\n",
      "736 [D loss: 0.616978, acc.: 81.25%] [G loss: 0.835088]\n",
      "737 [D loss: 0.560212, acc.: 81.25%] [G loss: 0.851382]\n",
      "738 [D loss: 0.600689, acc.: 81.25%] [G loss: 0.822434]\n",
      "739 [D loss: 0.686702, acc.: 59.38%] [G loss: 0.775621]\n",
      "740 [D loss: 0.620176, acc.: 53.12%] [G loss: 0.745580]\n",
      "741 [D loss: 0.648196, acc.: 59.38%] [G loss: 0.729215]\n",
      "742 [D loss: 0.635962, acc.: 65.62%] [G loss: 0.755062]\n",
      "743 [D loss: 0.629385, acc.: 62.50%] [G loss: 0.759395]\n",
      "744 [D loss: 0.640988, acc.: 62.50%] [G loss: 0.793495]\n",
      "745 [D loss: 0.623922, acc.: 65.62%] [G loss: 0.768367]\n",
      "746 [D loss: 0.601274, acc.: 65.62%] [G loss: 0.786781]\n",
      "747 [D loss: 0.680279, acc.: 56.25%] [G loss: 0.756044]\n",
      "748 [D loss: 0.662524, acc.: 59.38%] [G loss: 0.782337]\n",
      "749 [D loss: 0.637870, acc.: 56.25%] [G loss: 0.733526]\n",
      "750 [D loss: 0.682687, acc.: 56.25%] [G loss: 0.730439]\n",
      "751 [D loss: 0.633237, acc.: 59.38%] [G loss: 0.818009]\n",
      "752 [D loss: 0.606087, acc.: 71.88%] [G loss: 0.813268]\n",
      "753 [D loss: 0.665907, acc.: 56.25%] [G loss: 0.776223]\n",
      "754 [D loss: 0.668452, acc.: 59.38%] [G loss: 0.745787]\n",
      "755 [D loss: 0.698255, acc.: 43.75%] [G loss: 0.790682]\n",
      "756 [D loss: 0.616938, acc.: 68.75%] [G loss: 0.793449]\n",
      "757 [D loss: 0.704433, acc.: 40.62%] [G loss: 0.787520]\n",
      "758 [D loss: 0.688895, acc.: 46.88%] [G loss: 0.732007]\n",
      "759 [D loss: 0.683463, acc.: 56.25%] [G loss: 0.785141]\n",
      "760 [D loss: 0.708405, acc.: 40.62%] [G loss: 0.806426]\n",
      "761 [D loss: 0.671445, acc.: 50.00%] [G loss: 0.845337]\n",
      "762 [D loss: 0.657112, acc.: 62.50%] [G loss: 0.836442]\n",
      "763 [D loss: 0.704885, acc.: 53.12%] [G loss: 0.831661]\n",
      "764 [D loss: 0.645526, acc.: 68.75%] [G loss: 0.848046]\n",
      "765 [D loss: 0.620084, acc.: 75.00%] [G loss: 0.897164]\n",
      "766 [D loss: 0.676110, acc.: 53.12%] [G loss: 0.819505]\n",
      "767 [D loss: 0.607423, acc.: 71.88%] [G loss: 0.850881]\n",
      "768 [D loss: 0.685198, acc.: 62.50%] [G loss: 0.774506]\n",
      "769 [D loss: 0.703775, acc.: 43.75%] [G loss: 0.679419]\n",
      "770 [D loss: 0.677173, acc.: 53.12%] [G loss: 0.766407]\n",
      "771 [D loss: 0.629204, acc.: 56.25%] [G loss: 0.807755]\n",
      "772 [D loss: 0.606465, acc.: 71.88%] [G loss: 0.816908]\n",
      "773 [D loss: 0.647178, acc.: 68.75%] [G loss: 0.843423]\n",
      "774 [D loss: 0.588417, acc.: 68.75%] [G loss: 0.823343]\n",
      "775 [D loss: 0.657994, acc.: 68.75%] [G loss: 0.790471]\n",
      "776 [D loss: 0.616400, acc.: 65.62%] [G loss: 0.779467]\n",
      "777 [D loss: 0.610853, acc.: 75.00%] [G loss: 0.753914]\n",
      "778 [D loss: 0.658825, acc.: 65.62%] [G loss: 0.731098]\n",
      "779 [D loss: 0.653265, acc.: 56.25%] [G loss: 0.799620]\n",
      "780 [D loss: 0.620607, acc.: 59.38%] [G loss: 0.806512]\n",
      "781 [D loss: 0.644739, acc.: 59.38%] [G loss: 0.771812]\n",
      "782 [D loss: 0.636369, acc.: 56.25%] [G loss: 0.738727]\n",
      "783 [D loss: 0.630517, acc.: 62.50%] [G loss: 0.751820]\n",
      "784 [D loss: 0.580140, acc.: 71.88%] [G loss: 0.777701]\n",
      "785 [D loss: 0.621088, acc.: 68.75%] [G loss: 0.810889]\n",
      "786 [D loss: 0.637317, acc.: 71.88%] [G loss: 0.805948]\n",
      "787 [D loss: 0.601751, acc.: 68.75%] [G loss: 0.797748]\n",
      "788 [D loss: 0.660328, acc.: 56.25%] [G loss: 0.772678]\n",
      "789 [D loss: 0.614841, acc.: 62.50%] [G loss: 0.804540]\n",
      "790 [D loss: 0.614019, acc.: 81.25%] [G loss: 0.782143]\n",
      "791 [D loss: 0.595573, acc.: 78.12%] [G loss: 0.791678]\n",
      "792 [D loss: 0.632672, acc.: 62.50%] [G loss: 0.835484]\n",
      "793 [D loss: 0.643561, acc.: 62.50%] [G loss: 0.855640]\n",
      "794 [D loss: 0.649843, acc.: 62.50%] [G loss: 0.824124]\n",
      "795 [D loss: 0.613196, acc.: 75.00%] [G loss: 0.817686]\n",
      "796 [D loss: 0.674036, acc.: 62.50%] [G loss: 0.790911]\n",
      "797 [D loss: 0.628939, acc.: 59.38%] [G loss: 0.827160]\n",
      "798 [D loss: 0.650113, acc.: 59.38%] [G loss: 0.801400]\n",
      "799 [D loss: 0.614624, acc.: 68.75%] [G loss: 0.785482]\n",
      "800 [D loss: 0.645150, acc.: 53.12%] [G loss: 0.766093]\n",
      "801 [D loss: 0.655982, acc.: 56.25%] [G loss: 0.769022]\n",
      "802 [D loss: 0.628945, acc.: 65.62%] [G loss: 0.754940]\n",
      "803 [D loss: 0.588421, acc.: 68.75%] [G loss: 0.756455]\n",
      "804 [D loss: 0.575836, acc.: 78.12%] [G loss: 0.817185]\n",
      "805 [D loss: 0.644753, acc.: 59.38%] [G loss: 0.840987]\n",
      "806 [D loss: 0.587469, acc.: 81.25%] [G loss: 0.819361]\n",
      "807 [D loss: 0.651141, acc.: 65.62%] [G loss: 0.842057]\n",
      "808 [D loss: 0.685327, acc.: 43.75%] [G loss: 0.776363]\n",
      "809 [D loss: 0.633687, acc.: 65.62%] [G loss: 0.773489]\n",
      "810 [D loss: 0.653888, acc.: 65.62%] [G loss: 0.789119]\n",
      "811 [D loss: 0.641859, acc.: 59.38%] [G loss: 0.810617]\n",
      "812 [D loss: 0.671144, acc.: 53.12%] [G loss: 0.797639]\n",
      "813 [D loss: 0.631946, acc.: 71.88%] [G loss: 0.787094]\n",
      "814 [D loss: 0.623301, acc.: 68.75%] [G loss: 0.763317]\n",
      "815 [D loss: 0.659261, acc.: 50.00%] [G loss: 0.764391]\n",
      "816 [D loss: 0.625061, acc.: 59.38%] [G loss: 0.847671]\n",
      "817 [D loss: 0.655069, acc.: 46.88%] [G loss: 0.838057]\n",
      "818 [D loss: 0.664624, acc.: 50.00%] [G loss: 0.794283]\n",
      "819 [D loss: 0.653904, acc.: 56.25%] [G loss: 0.776435]\n",
      "820 [D loss: 0.600712, acc.: 75.00%] [G loss: 0.754604]\n",
      "821 [D loss: 0.628200, acc.: 59.38%] [G loss: 0.758979]\n",
      "822 [D loss: 0.641925, acc.: 68.75%] [G loss: 0.766753]\n",
      "823 [D loss: 0.636562, acc.: 53.12%] [G loss: 0.795709]\n",
      "824 [D loss: 0.601341, acc.: 71.88%] [G loss: 0.779326]\n",
      "825 [D loss: 0.637879, acc.: 68.75%] [G loss: 0.765400]\n",
      "826 [D loss: 0.618806, acc.: 71.88%] [G loss: 0.775369]\n",
      "827 [D loss: 0.588200, acc.: 75.00%] [G loss: 0.750889]\n",
      "828 [D loss: 0.641972, acc.: 59.38%] [G loss: 0.779626]\n",
      "829 [D loss: 0.709342, acc.: 46.88%] [G loss: 0.781682]\n",
      "830 [D loss: 0.647742, acc.: 56.25%] [G loss: 0.799865]\n",
      "831 [D loss: 0.640495, acc.: 68.75%] [G loss: 0.800197]\n",
      "832 [D loss: 0.640445, acc.: 56.25%] [G loss: 0.816392]\n",
      "833 [D loss: 0.660316, acc.: 50.00%] [G loss: 0.756270]\n",
      "834 [D loss: 0.650398, acc.: 56.25%] [G loss: 0.771991]\n",
      "835 [D loss: 0.634978, acc.: 65.62%] [G loss: 0.765076]\n",
      "836 [D loss: 0.590715, acc.: 71.88%] [G loss: 0.802788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837 [D loss: 0.618421, acc.: 71.88%] [G loss: 0.798161]\n",
      "838 [D loss: 0.601476, acc.: 71.88%] [G loss: 0.751755]\n",
      "839 [D loss: 0.609123, acc.: 65.62%] [G loss: 0.750642]\n",
      "840 [D loss: 0.648579, acc.: 62.50%] [G loss: 0.721334]\n",
      "841 [D loss: 0.624920, acc.: 62.50%] [G loss: 0.754077]\n",
      "842 [D loss: 0.676951, acc.: 56.25%] [G loss: 0.736555]\n",
      "843 [D loss: 0.614446, acc.: 65.62%] [G loss: 0.796273]\n",
      "844 [D loss: 0.602010, acc.: 65.62%] [G loss: 0.820986]\n",
      "845 [D loss: 0.641445, acc.: 68.75%] [G loss: 0.796329]\n",
      "846 [D loss: 0.742968, acc.: 46.88%] [G loss: 0.799046]\n",
      "847 [D loss: 0.656263, acc.: 56.25%] [G loss: 0.754691]\n",
      "848 [D loss: 0.640105, acc.: 59.38%] [G loss: 0.795985]\n",
      "849 [D loss: 0.713675, acc.: 50.00%] [G loss: 0.799579]\n",
      "850 [D loss: 0.665909, acc.: 53.12%] [G loss: 0.840808]\n",
      "851 [D loss: 0.658139, acc.: 56.25%] [G loss: 0.804728]\n",
      "852 [D loss: 0.696178, acc.: 56.25%] [G loss: 0.808299]\n",
      "853 [D loss: 0.621680, acc.: 56.25%] [G loss: 0.863379]\n",
      "854 [D loss: 0.692817, acc.: 56.25%] [G loss: 0.879257]\n",
      "855 [D loss: 0.682291, acc.: 53.12%] [G loss: 0.820626]\n",
      "856 [D loss: 0.632103, acc.: 59.38%] [G loss: 0.871162]\n",
      "857 [D loss: 0.615826, acc.: 62.50%] [G loss: 0.931979]\n",
      "858 [D loss: 0.664155, acc.: 62.50%] [G loss: 0.853034]\n",
      "859 [D loss: 0.637268, acc.: 59.38%] [G loss: 0.798487]\n",
      "860 [D loss: 0.644194, acc.: 68.75%] [G loss: 0.856928]\n",
      "861 [D loss: 0.650055, acc.: 62.50%] [G loss: 0.849888]\n",
      "862 [D loss: 0.641084, acc.: 56.25%] [G loss: 0.827130]\n",
      "863 [D loss: 0.648919, acc.: 53.12%] [G loss: 0.838954]\n",
      "864 [D loss: 0.648509, acc.: 53.12%] [G loss: 0.829704]\n",
      "865 [D loss: 0.650784, acc.: 62.50%] [G loss: 0.861640]\n",
      "866 [D loss: 0.635414, acc.: 65.62%] [G loss: 0.856226]\n",
      "867 [D loss: 0.630806, acc.: 75.00%] [G loss: 0.818502]\n",
      "868 [D loss: 0.620577, acc.: 71.88%] [G loss: 0.810134]\n",
      "869 [D loss: 0.618999, acc.: 71.88%] [G loss: 0.784351]\n",
      "870 [D loss: 0.633439, acc.: 65.62%] [G loss: 0.792103]\n",
      "871 [D loss: 0.710286, acc.: 34.38%] [G loss: 0.768883]\n",
      "872 [D loss: 0.622109, acc.: 62.50%] [G loss: 0.770773]\n",
      "873 [D loss: 0.640898, acc.: 65.62%] [G loss: 0.798209]\n",
      "874 [D loss: 0.610302, acc.: 65.62%] [G loss: 0.825787]\n",
      "875 [D loss: 0.640584, acc.: 71.88%] [G loss: 0.859496]\n",
      "876 [D loss: 0.610642, acc.: 59.38%] [G loss: 0.820733]\n",
      "877 [D loss: 0.651060, acc.: 65.62%] [G loss: 0.870498]\n",
      "878 [D loss: 0.647796, acc.: 62.50%] [G loss: 0.855597]\n",
      "879 [D loss: 0.574905, acc.: 84.38%] [G loss: 0.810708]\n",
      "880 [D loss: 0.638033, acc.: 78.12%] [G loss: 0.808824]\n",
      "881 [D loss: 0.631128, acc.: 62.50%] [G loss: 0.862228]\n",
      "882 [D loss: 0.596299, acc.: 78.12%] [G loss: 0.869558]\n",
      "883 [D loss: 0.627199, acc.: 78.12%] [G loss: 0.821419]\n",
      "884 [D loss: 0.611821, acc.: 71.88%] [G loss: 0.817630]\n",
      "885 [D loss: 0.611450, acc.: 68.75%] [G loss: 0.749643]\n",
      "886 [D loss: 0.644801, acc.: 68.75%] [G loss: 0.828207]\n",
      "887 [D loss: 0.604140, acc.: 62.50%] [G loss: 0.852974]\n",
      "888 [D loss: 0.749534, acc.: 37.50%] [G loss: 0.842759]\n",
      "889 [D loss: 0.660296, acc.: 56.25%] [G loss: 0.839001]\n",
      "890 [D loss: 0.632253, acc.: 68.75%] [G loss: 0.787252]\n",
      "891 [D loss: 0.630052, acc.: 59.38%] [G loss: 0.786201]\n",
      "892 [D loss: 0.598071, acc.: 65.62%] [G loss: 0.840697]\n",
      "893 [D loss: 0.643016, acc.: 68.75%] [G loss: 0.835013]\n",
      "894 [D loss: 0.598076, acc.: 75.00%] [G loss: 0.866555]\n",
      "895 [D loss: 0.593767, acc.: 75.00%] [G loss: 0.815017]\n",
      "896 [D loss: 0.666078, acc.: 62.50%] [G loss: 0.790212]\n",
      "897 [D loss: 0.648209, acc.: 65.62%] [G loss: 0.799484]\n",
      "898 [D loss: 0.608906, acc.: 65.62%] [G loss: 0.803800]\n",
      "899 [D loss: 0.640356, acc.: 71.88%] [G loss: 0.739357]\n",
      "900 [D loss: 0.668573, acc.: 59.38%] [G loss: 0.735017]\n",
      "901 [D loss: 0.624716, acc.: 71.88%] [G loss: 0.768627]\n",
      "902 [D loss: 0.603766, acc.: 71.88%] [G loss: 0.827935]\n",
      "903 [D loss: 0.644452, acc.: 59.38%] [G loss: 0.880704]\n",
      "904 [D loss: 0.731376, acc.: 43.75%] [G loss: 0.814157]\n",
      "905 [D loss: 0.643033, acc.: 68.75%] [G loss: 0.787629]\n",
      "906 [D loss: 0.654644, acc.: 50.00%] [G loss: 0.736336]\n",
      "907 [D loss: 0.609617, acc.: 78.12%] [G loss: 0.780375]\n",
      "908 [D loss: 0.608880, acc.: 78.12%] [G loss: 0.794354]\n",
      "909 [D loss: 0.616949, acc.: 78.12%] [G loss: 0.733250]\n",
      "910 [D loss: 0.611700, acc.: 65.62%] [G loss: 0.736544]\n",
      "911 [D loss: 0.670025, acc.: 65.62%] [G loss: 0.720505]\n",
      "912 [D loss: 0.577545, acc.: 71.88%] [G loss: 0.761243]\n",
      "913 [D loss: 0.627526, acc.: 68.75%] [G loss: 0.813916]\n",
      "914 [D loss: 0.658508, acc.: 53.12%] [G loss: 0.760678]\n",
      "915 [D loss: 0.616452, acc.: 62.50%] [G loss: 0.725916]\n",
      "916 [D loss: 0.609470, acc.: 62.50%] [G loss: 0.810322]\n",
      "917 [D loss: 0.640139, acc.: 65.62%] [G loss: 0.840182]\n",
      "918 [D loss: 0.616743, acc.: 71.88%] [G loss: 0.854972]\n",
      "919 [D loss: 0.633153, acc.: 62.50%] [G loss: 0.839837]\n",
      "920 [D loss: 0.607871, acc.: 71.88%] [G loss: 0.842541]\n",
      "921 [D loss: 0.634270, acc.: 71.88%] [G loss: 0.844042]\n",
      "922 [D loss: 0.623456, acc.: 68.75%] [G loss: 0.793064]\n",
      "923 [D loss: 0.682326, acc.: 53.12%] [G loss: 0.829554]\n",
      "924 [D loss: 0.644635, acc.: 59.38%] [G loss: 0.848853]\n",
      "925 [D loss: 0.662314, acc.: 59.38%] [G loss: 0.795883]\n",
      "926 [D loss: 0.637707, acc.: 71.88%] [G loss: 0.733668]\n",
      "927 [D loss: 0.651154, acc.: 65.62%] [G loss: 0.743676]\n",
      "928 [D loss: 0.677113, acc.: 65.62%] [G loss: 0.778483]\n",
      "929 [D loss: 0.641506, acc.: 53.12%] [G loss: 0.795462]\n",
      "930 [D loss: 0.632549, acc.: 65.62%] [G loss: 0.799463]\n",
      "931 [D loss: 0.640056, acc.: 68.75%] [G loss: 0.816749]\n",
      "932 [D loss: 0.612934, acc.: 75.00%] [G loss: 0.863776]\n",
      "933 [D loss: 0.622266, acc.: 71.88%] [G loss: 0.808970]\n",
      "934 [D loss: 0.675977, acc.: 50.00%] [G loss: 0.749981]\n",
      "935 [D loss: 0.665290, acc.: 50.00%] [G loss: 0.793111]\n",
      "936 [D loss: 0.576660, acc.: 75.00%] [G loss: 0.861803]\n",
      "937 [D loss: 0.624296, acc.: 71.88%] [G loss: 0.832087]\n",
      "938 [D loss: 0.686657, acc.: 53.12%] [G loss: 0.797668]\n",
      "939 [D loss: 0.639951, acc.: 62.50%] [G loss: 0.814312]\n",
      "940 [D loss: 0.655398, acc.: 59.38%] [G loss: 0.773280]\n",
      "941 [D loss: 0.658342, acc.: 50.00%] [G loss: 0.814429]\n",
      "942 [D loss: 0.633146, acc.: 59.38%] [G loss: 0.805790]\n",
      "943 [D loss: 0.603568, acc.: 75.00%] [G loss: 0.838084]\n",
      "944 [D loss: 0.618413, acc.: 71.88%] [G loss: 0.839040]\n",
      "945 [D loss: 0.637184, acc.: 62.50%] [G loss: 0.805007]\n",
      "946 [D loss: 0.634172, acc.: 68.75%] [G loss: 0.740269]\n",
      "947 [D loss: 0.641415, acc.: 68.75%] [G loss: 0.751665]\n",
      "948 [D loss: 0.681051, acc.: 50.00%] [G loss: 0.776762]\n",
      "949 [D loss: 0.658486, acc.: 59.38%] [G loss: 0.764300]\n",
      "950 [D loss: 0.661525, acc.: 59.38%] [G loss: 0.744865]\n",
      "951 [D loss: 0.646181, acc.: 71.88%] [G loss: 0.759975]\n",
      "952 [D loss: 0.658944, acc.: 71.88%] [G loss: 0.755840]\n",
      "953 [D loss: 0.592829, acc.: 78.12%] [G loss: 0.829795]\n",
      "954 [D loss: 0.583169, acc.: 75.00%] [G loss: 0.838124]\n",
      "955 [D loss: 0.644646, acc.: 62.50%] [G loss: 0.826149]\n",
      "956 [D loss: 0.640149, acc.: 65.62%] [G loss: 0.831907]\n",
      "957 [D loss: 0.643779, acc.: 78.12%] [G loss: 0.849230]\n",
      "958 [D loss: 0.599678, acc.: 75.00%] [G loss: 0.845364]\n",
      "959 [D loss: 0.591441, acc.: 68.75%] [G loss: 0.823395]\n",
      "960 [D loss: 0.632008, acc.: 65.62%] [G loss: 0.787189]\n",
      "961 [D loss: 0.597561, acc.: 68.75%] [G loss: 0.838516]\n",
      "962 [D loss: 0.623779, acc.: 65.62%] [G loss: 0.861524]\n",
      "963 [D loss: 0.594746, acc.: 75.00%] [G loss: 0.853276]\n",
      "964 [D loss: 0.620158, acc.: 62.50%] [G loss: 0.818618]\n",
      "965 [D loss: 0.638803, acc.: 65.62%] [G loss: 0.777101]\n",
      "966 [D loss: 0.631066, acc.: 62.50%] [G loss: 0.789130]\n",
      "967 [D loss: 0.639542, acc.: 62.50%] [G loss: 0.824233]\n",
      "968 [D loss: 0.665424, acc.: 53.12%] [G loss: 0.801082]\n",
      "969 [D loss: 0.591527, acc.: 75.00%] [G loss: 0.821696]\n",
      "970 [D loss: 0.658978, acc.: 53.12%] [G loss: 0.856044]\n",
      "971 [D loss: 0.620689, acc.: 65.62%] [G loss: 0.809968]\n",
      "972 [D loss: 0.625385, acc.: 68.75%] [G loss: 0.756272]\n",
      "973 [D loss: 0.638942, acc.: 68.75%] [G loss: 0.825336]\n",
      "974 [D loss: 0.641556, acc.: 62.50%] [G loss: 0.826025]\n",
      "975 [D loss: 0.525381, acc.: 75.00%] [G loss: 0.883176]\n",
      "976 [D loss: 0.726948, acc.: 50.00%] [G loss: 0.844094]\n",
      "977 [D loss: 0.678192, acc.: 56.25%] [G loss: 0.844311]\n",
      "978 [D loss: 0.608643, acc.: 62.50%] [G loss: 0.814396]\n",
      "979 [D loss: 0.635596, acc.: 65.62%] [G loss: 0.776353]\n",
      "980 [D loss: 0.682220, acc.: 65.62%] [G loss: 0.815007]\n",
      "981 [D loss: 0.662110, acc.: 56.25%] [G loss: 0.781034]\n",
      "982 [D loss: 0.623946, acc.: 56.25%] [G loss: 0.777704]\n",
      "983 [D loss: 0.586988, acc.: 78.12%] [G loss: 0.814141]\n",
      "984 [D loss: 0.581280, acc.: 68.75%] [G loss: 0.848790]\n",
      "985 [D loss: 0.648743, acc.: 59.38%] [G loss: 0.906485]\n",
      "986 [D loss: 0.620393, acc.: 81.25%] [G loss: 0.847954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987 [D loss: 0.638605, acc.: 59.38%] [G loss: 0.843406]\n",
      "988 [D loss: 0.611120, acc.: 68.75%] [G loss: 0.893775]\n",
      "989 [D loss: 0.640271, acc.: 65.62%] [G loss: 0.850550]\n",
      "990 [D loss: 0.594886, acc.: 68.75%] [G loss: 0.923480]\n",
      "991 [D loss: 0.612628, acc.: 71.88%] [G loss: 0.890976]\n",
      "992 [D loss: 0.596472, acc.: 81.25%] [G loss: 0.868062]\n",
      "993 [D loss: 0.600325, acc.: 68.75%] [G loss: 0.857983]\n",
      "994 [D loss: 0.588368, acc.: 71.88%] [G loss: 0.855970]\n",
      "995 [D loss: 0.560931, acc.: 78.12%] [G loss: 0.774584]\n",
      "996 [D loss: 0.600967, acc.: 68.75%] [G loss: 0.798013]\n",
      "997 [D loss: 0.685447, acc.: 53.12%] [G loss: 0.833126]\n",
      "998 [D loss: 0.625215, acc.: 62.50%] [G loss: 0.860195]\n",
      "999 [D loss: 0.650963, acc.: 56.25%] [G loss: 0.833515]\n",
      "1000 [D loss: 0.681837, acc.: 50.00%] [G loss: 0.864061]\n",
      "1001 [D loss: 0.553079, acc.: 81.25%] [G loss: 0.889839]\n",
      "1002 [D loss: 0.689588, acc.: 50.00%] [G loss: 0.825954]\n",
      "1003 [D loss: 0.686582, acc.: 50.00%] [G loss: 0.763782]\n",
      "1004 [D loss: 0.623317, acc.: 65.62%] [G loss: 0.751782]\n",
      "1005 [D loss: 0.621551, acc.: 62.50%] [G loss: 0.783642]\n",
      "1006 [D loss: 0.594369, acc.: 71.88%] [G loss: 0.799940]\n",
      "1007 [D loss: 0.563271, acc.: 78.12%] [G loss: 0.785469]\n",
      "1008 [D loss: 0.632614, acc.: 78.12%] [G loss: 0.783911]\n",
      "1009 [D loss: 0.604887, acc.: 75.00%] [G loss: 0.810563]\n",
      "1010 [D loss: 0.673668, acc.: 53.12%] [G loss: 0.797502]\n",
      "1011 [D loss: 0.580084, acc.: 84.38%] [G loss: 0.830981]\n",
      "1012 [D loss: 0.641293, acc.: 65.62%] [G loss: 0.832371]\n",
      "1013 [D loss: 0.660717, acc.: 65.62%] [G loss: 0.821735]\n",
      "1014 [D loss: 0.666600, acc.: 62.50%] [G loss: 0.782421]\n",
      "1015 [D loss: 0.530174, acc.: 96.88%] [G loss: 0.820497]\n",
      "1016 [D loss: 0.586168, acc.: 75.00%] [G loss: 0.803922]\n",
      "1017 [D loss: 0.632439, acc.: 62.50%] [G loss: 0.758673]\n",
      "1018 [D loss: 0.626441, acc.: 56.25%] [G loss: 0.822451]\n",
      "1019 [D loss: 0.596354, acc.: 71.88%] [G loss: 0.836280]\n",
      "1020 [D loss: 0.579280, acc.: 75.00%] [G loss: 0.805628]\n",
      "1021 [D loss: 0.637821, acc.: 65.62%] [G loss: 0.802768]\n",
      "1022 [D loss: 0.617173, acc.: 71.88%] [G loss: 0.877967]\n",
      "1023 [D loss: 0.638989, acc.: 62.50%] [G loss: 0.861564]\n",
      "1024 [D loss: 0.599278, acc.: 75.00%] [G loss: 0.831862]\n",
      "1025 [D loss: 0.641698, acc.: 65.62%] [G loss: 0.836718]\n",
      "1026 [D loss: 0.584135, acc.: 71.88%] [G loss: 0.856536]\n",
      "1027 [D loss: 0.560344, acc.: 81.25%] [G loss: 0.837353]\n",
      "1028 [D loss: 0.696344, acc.: 50.00%] [G loss: 0.808946]\n",
      "1029 [D loss: 0.658687, acc.: 50.00%] [G loss: 0.774439]\n",
      "1030 [D loss: 0.617849, acc.: 59.38%] [G loss: 0.827789]\n",
      "1031 [D loss: 0.616750, acc.: 65.62%] [G loss: 0.834600]\n",
      "1032 [D loss: 0.645488, acc.: 62.50%] [G loss: 0.809479]\n",
      "1033 [D loss: 0.641350, acc.: 46.88%] [G loss: 0.778356]\n",
      "1034 [D loss: 0.593427, acc.: 78.12%] [G loss: 0.829188]\n",
      "1035 [D loss: 0.646160, acc.: 56.25%] [G loss: 0.797991]\n",
      "1036 [D loss: 0.617625, acc.: 65.62%] [G loss: 0.848671]\n",
      "1037 [D loss: 0.607141, acc.: 68.75%] [G loss: 0.821567]\n",
      "1038 [D loss: 0.697380, acc.: 50.00%] [G loss: 0.820615]\n",
      "1039 [D loss: 0.606873, acc.: 71.88%] [G loss: 0.862249]\n",
      "1040 [D loss: 0.639884, acc.: 65.62%] [G loss: 0.843980]\n",
      "1041 [D loss: 0.680448, acc.: 53.12%] [G loss: 0.873155]\n",
      "1042 [D loss: 0.602990, acc.: 68.75%] [G loss: 0.855048]\n",
      "1043 [D loss: 0.616368, acc.: 71.88%] [G loss: 0.799816]\n",
      "1044 [D loss: 0.618966, acc.: 68.75%] [G loss: 0.787886]\n",
      "1045 [D loss: 0.613375, acc.: 59.38%] [G loss: 0.822527]\n",
      "1046 [D loss: 0.570312, acc.: 71.88%] [G loss: 0.808622]\n",
      "1047 [D loss: 0.692765, acc.: 56.25%] [G loss: 0.785547]\n",
      "1048 [D loss: 0.622784, acc.: 46.88%] [G loss: 0.795808]\n",
      "1049 [D loss: 0.612696, acc.: 65.62%] [G loss: 0.828182]\n",
      "1050 [D loss: 0.649905, acc.: 62.50%] [G loss: 0.823569]\n",
      "1051 [D loss: 0.680094, acc.: 59.38%] [G loss: 0.814433]\n",
      "1052 [D loss: 0.644121, acc.: 59.38%] [G loss: 0.823875]\n",
      "1053 [D loss: 0.609634, acc.: 81.25%] [G loss: 0.797191]\n",
      "1054 [D loss: 0.686809, acc.: 59.38%] [G loss: 0.782517]\n",
      "1055 [D loss: 0.613540, acc.: 59.38%] [G loss: 0.822400]\n",
      "1056 [D loss: 0.631161, acc.: 62.50%] [G loss: 0.888520]\n",
      "1057 [D loss: 0.645730, acc.: 59.38%] [G loss: 0.786920]\n",
      "1058 [D loss: 0.589705, acc.: 81.25%] [G loss: 0.813668]\n",
      "1059 [D loss: 0.623821, acc.: 65.62%] [G loss: 0.780824]\n",
      "1060 [D loss: 0.645446, acc.: 59.38%] [G loss: 0.830887]\n",
      "1061 [D loss: 0.677324, acc.: 43.75%] [G loss: 0.824858]\n",
      "1062 [D loss: 0.655023, acc.: 65.62%] [G loss: 0.869184]\n",
      "1063 [D loss: 0.629840, acc.: 65.62%] [G loss: 0.816784]\n",
      "1064 [D loss: 0.631335, acc.: 59.38%] [G loss: 0.784568]\n",
      "1065 [D loss: 0.720856, acc.: 53.12%] [G loss: 0.843837]\n",
      "1066 [D loss: 0.604796, acc.: 87.50%] [G loss: 0.822805]\n",
      "1067 [D loss: 0.606758, acc.: 65.62%] [G loss: 0.846682]\n",
      "1068 [D loss: 0.681998, acc.: 50.00%] [G loss: 0.789760]\n",
      "1069 [D loss: 0.612451, acc.: 75.00%] [G loss: 0.864084]\n",
      "1070 [D loss: 0.662093, acc.: 59.38%] [G loss: 0.800900]\n",
      "1071 [D loss: 0.629268, acc.: 71.88%] [G loss: 0.793948]\n",
      "1072 [D loss: 0.608207, acc.: 68.75%] [G loss: 0.822444]\n",
      "1073 [D loss: 0.640492, acc.: 59.38%] [G loss: 0.818715]\n",
      "1074 [D loss: 0.623935, acc.: 68.75%] [G loss: 0.816573]\n",
      "1075 [D loss: 0.615387, acc.: 59.38%] [G loss: 0.787974]\n",
      "1076 [D loss: 0.648599, acc.: 65.62%] [G loss: 0.758956]\n",
      "1077 [D loss: 0.632877, acc.: 65.62%] [G loss: 0.780637]\n",
      "1078 [D loss: 0.577412, acc.: 68.75%] [G loss: 0.844519]\n",
      "1079 [D loss: 0.594995, acc.: 71.88%] [G loss: 0.841897]\n",
      "1080 [D loss: 0.652064, acc.: 59.38%] [G loss: 0.851665]\n",
      "1081 [D loss: 0.656347, acc.: 62.50%] [G loss: 0.836003]\n",
      "1082 [D loss: 0.667152, acc.: 62.50%] [G loss: 0.790110]\n",
      "1083 [D loss: 0.690745, acc.: 56.25%] [G loss: 0.778278]\n",
      "1084 [D loss: 0.656336, acc.: 62.50%] [G loss: 0.755174]\n",
      "1085 [D loss: 0.621997, acc.: 71.88%] [G loss: 0.780940]\n",
      "1086 [D loss: 0.576812, acc.: 78.12%] [G loss: 0.775130]\n",
      "1087 [D loss: 0.605466, acc.: 68.75%] [G loss: 0.809869]\n",
      "1088 [D loss: 0.593783, acc.: 65.62%] [G loss: 0.835451]\n",
      "1089 [D loss: 0.641347, acc.: 62.50%] [G loss: 0.827501]\n",
      "1090 [D loss: 0.633550, acc.: 71.88%] [G loss: 0.732970]\n",
      "1091 [D loss: 0.583102, acc.: 65.62%] [G loss: 0.743229]\n",
      "1092 [D loss: 0.588240, acc.: 75.00%] [G loss: 0.779500]\n",
      "1093 [D loss: 0.669080, acc.: 53.12%] [G loss: 0.762605]\n",
      "1094 [D loss: 0.604732, acc.: 75.00%] [G loss: 0.784222]\n",
      "1095 [D loss: 0.671196, acc.: 53.12%] [G loss: 0.822685]\n",
      "1096 [D loss: 0.639664, acc.: 68.75%] [G loss: 0.866885]\n",
      "1097 [D loss: 0.624851, acc.: 59.38%] [G loss: 0.863051]\n",
      "1098 [D loss: 0.648620, acc.: 59.38%] [G loss: 0.823840]\n",
      "1099 [D loss: 0.667578, acc.: 59.38%] [G loss: 0.779605]\n",
      "1100 [D loss: 0.586376, acc.: 68.75%] [G loss: 0.847024]\n",
      "1101 [D loss: 0.589663, acc.: 81.25%] [G loss: 0.853678]\n",
      "1102 [D loss: 0.553787, acc.: 81.25%] [G loss: 0.922984]\n",
      "1103 [D loss: 0.662941, acc.: 62.50%] [G loss: 0.783238]\n",
      "1104 [D loss: 0.622670, acc.: 59.38%] [G loss: 0.766019]\n",
      "1105 [D loss: 0.646213, acc.: 62.50%] [G loss: 0.764782]\n",
      "1106 [D loss: 0.631547, acc.: 59.38%] [G loss: 0.756166]\n",
      "1107 [D loss: 0.586356, acc.: 65.62%] [G loss: 0.803273]\n",
      "1108 [D loss: 0.630346, acc.: 59.38%] [G loss: 0.800549]\n",
      "1109 [D loss: 0.635522, acc.: 62.50%] [G loss: 0.845737]\n",
      "1110 [D loss: 0.569534, acc.: 87.50%] [G loss: 0.846175]\n",
      "1111 [D loss: 0.613646, acc.: 71.88%] [G loss: 0.800692]\n",
      "1112 [D loss: 0.631870, acc.: 65.62%] [G loss: 0.789877]\n",
      "1113 [D loss: 0.664210, acc.: 50.00%] [G loss: 0.834553]\n",
      "1114 [D loss: 0.638684, acc.: 65.62%] [G loss: 0.835940]\n",
      "1115 [D loss: 0.621800, acc.: 65.62%] [G loss: 0.800091]\n",
      "1116 [D loss: 0.603544, acc.: 62.50%] [G loss: 0.817403]\n",
      "1117 [D loss: 0.658236, acc.: 53.12%] [G loss: 0.874783]\n",
      "1118 [D loss: 0.659259, acc.: 68.75%] [G loss: 0.852394]\n",
      "1119 [D loss: 0.664925, acc.: 56.25%] [G loss: 0.861110]\n",
      "1120 [D loss: 0.586164, acc.: 62.50%] [G loss: 0.895465]\n",
      "1121 [D loss: 0.601731, acc.: 68.75%] [G loss: 0.854043]\n",
      "1122 [D loss: 0.670442, acc.: 53.12%] [G loss: 0.830148]\n",
      "1123 [D loss: 0.694395, acc.: 53.12%] [G loss: 0.832769]\n",
      "1124 [D loss: 0.714100, acc.: 43.75%] [G loss: 0.828255]\n",
      "1125 [D loss: 0.615730, acc.: 68.75%] [G loss: 0.821123]\n",
      "1126 [D loss: 0.614015, acc.: 71.88%] [G loss: 0.856981]\n",
      "1127 [D loss: 0.593694, acc.: 71.88%] [G loss: 0.888228]\n",
      "1128 [D loss: 0.596411, acc.: 75.00%] [G loss: 0.787288]\n",
      "1129 [D loss: 0.627122, acc.: 71.88%] [G loss: 0.760496]\n",
      "1130 [D loss: 0.627250, acc.: 65.62%] [G loss: 0.744205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1131 [D loss: 0.698449, acc.: 62.50%] [G loss: 0.801497]\n",
      "1132 [D loss: 0.670707, acc.: 56.25%] [G loss: 0.862657]\n",
      "1133 [D loss: 0.581995, acc.: 71.88%] [G loss: 0.863113]\n",
      "1134 [D loss: 0.601727, acc.: 81.25%] [G loss: 0.832679]\n",
      "1135 [D loss: 0.637979, acc.: 50.00%] [G loss: 0.863377]\n",
      "1136 [D loss: 0.578908, acc.: 81.25%] [G loss: 0.919656]\n",
      "1137 [D loss: 0.628228, acc.: 68.75%] [G loss: 0.886720]\n",
      "1138 [D loss: 0.655071, acc.: 53.12%] [G loss: 0.774211]\n",
      "1139 [D loss: 0.656370, acc.: 59.38%] [G loss: 0.900189]\n",
      "1140 [D loss: 0.618070, acc.: 65.62%] [G loss: 0.884907]\n",
      "1141 [D loss: 0.659460, acc.: 56.25%] [G loss: 0.826726]\n",
      "1142 [D loss: 0.607455, acc.: 75.00%] [G loss: 0.847278]\n",
      "1143 [D loss: 0.632393, acc.: 65.62%] [G loss: 0.821788]\n",
      "1144 [D loss: 0.607456, acc.: 68.75%] [G loss: 0.857083]\n",
      "1145 [D loss: 0.697625, acc.: 50.00%] [G loss: 0.773063]\n",
      "1146 [D loss: 0.628299, acc.: 62.50%] [G loss: 0.802879]\n",
      "1147 [D loss: 0.704164, acc.: 46.88%] [G loss: 0.769068]\n",
      "1148 [D loss: 0.652830, acc.: 59.38%] [G loss: 0.783027]\n",
      "1149 [D loss: 0.605483, acc.: 68.75%] [G loss: 0.792641]\n",
      "1150 [D loss: 0.609110, acc.: 68.75%] [G loss: 0.820904]\n",
      "1151 [D loss: 0.591798, acc.: 75.00%] [G loss: 0.856281]\n",
      "1152 [D loss: 0.577347, acc.: 71.88%] [G loss: 0.855269]\n",
      "1153 [D loss: 0.640824, acc.: 65.62%] [G loss: 0.807346]\n",
      "1154 [D loss: 0.663656, acc.: 53.12%] [G loss: 0.850560]\n",
      "1155 [D loss: 0.627896, acc.: 62.50%] [G loss: 0.839109]\n",
      "1156 [D loss: 0.543617, acc.: 81.25%] [G loss: 0.853561]\n",
      "1157 [D loss: 0.574485, acc.: 78.12%] [G loss: 0.833748]\n",
      "1158 [D loss: 0.622686, acc.: 78.12%] [G loss: 0.858332]\n",
      "1159 [D loss: 0.561987, acc.: 93.75%] [G loss: 0.860461]\n",
      "1160 [D loss: 0.619442, acc.: 65.62%] [G loss: 0.827166]\n",
      "1161 [D loss: 0.600723, acc.: 65.62%] [G loss: 0.865100]\n",
      "1162 [D loss: 0.585030, acc.: 75.00%] [G loss: 0.904267]\n",
      "1163 [D loss: 0.539039, acc.: 84.38%] [G loss: 0.906162]\n",
      "1164 [D loss: 0.657318, acc.: 65.62%] [G loss: 0.914172]\n",
      "1165 [D loss: 0.662911, acc.: 75.00%] [G loss: 0.791288]\n",
      "1166 [D loss: 0.638802, acc.: 56.25%] [G loss: 0.840209]\n",
      "1167 [D loss: 0.605757, acc.: 68.75%] [G loss: 0.818010]\n",
      "1168 [D loss: 0.607043, acc.: 71.88%] [G loss: 0.848438]\n",
      "1169 [D loss: 0.632238, acc.: 62.50%] [G loss: 0.819793]\n",
      "1170 [D loss: 0.639187, acc.: 68.75%] [G loss: 0.824353]\n",
      "1171 [D loss: 0.647202, acc.: 56.25%] [G loss: 0.804725]\n",
      "1172 [D loss: 0.642557, acc.: 68.75%] [G loss: 0.823873]\n",
      "1173 [D loss: 0.659874, acc.: 65.62%] [G loss: 0.894987]\n",
      "1174 [D loss: 0.608373, acc.: 62.50%] [G loss: 0.872537]\n",
      "1175 [D loss: 0.597154, acc.: 71.88%] [G loss: 0.872820]\n",
      "1176 [D loss: 0.630699, acc.: 68.75%] [G loss: 0.821978]\n",
      "1177 [D loss: 0.640960, acc.: 68.75%] [G loss: 0.829713]\n",
      "1178 [D loss: 0.591856, acc.: 81.25%] [G loss: 0.847835]\n",
      "1179 [D loss: 0.671256, acc.: 56.25%] [G loss: 0.926350]\n",
      "1180 [D loss: 0.605655, acc.: 81.25%] [G loss: 0.870036]\n",
      "1181 [D loss: 0.690046, acc.: 59.38%] [G loss: 0.824459]\n",
      "1182 [D loss: 0.610133, acc.: 71.88%] [G loss: 0.791964]\n",
      "1183 [D loss: 0.634991, acc.: 62.50%] [G loss: 0.764864]\n",
      "1184 [D loss: 0.613702, acc.: 68.75%] [G loss: 0.816481]\n",
      "1185 [D loss: 0.621335, acc.: 81.25%] [G loss: 0.839184]\n",
      "1186 [D loss: 0.651607, acc.: 53.12%] [G loss: 0.831877]\n",
      "1187 [D loss: 0.637153, acc.: 65.62%] [G loss: 0.880073]\n",
      "1188 [D loss: 0.637566, acc.: 56.25%] [G loss: 0.877690]\n",
      "1189 [D loss: 0.631413, acc.: 59.38%] [G loss: 0.871892]\n",
      "1190 [D loss: 0.595041, acc.: 75.00%] [G loss: 0.909598]\n",
      "1191 [D loss: 0.622779, acc.: 68.75%] [G loss: 0.904468]\n",
      "1192 [D loss: 0.644471, acc.: 62.50%] [G loss: 0.845054]\n",
      "1193 [D loss: 0.680126, acc.: 62.50%] [G loss: 0.756938]\n",
      "1194 [D loss: 0.572779, acc.: 68.75%] [G loss: 0.815141]\n",
      "1195 [D loss: 0.591717, acc.: 65.62%] [G loss: 0.826451]\n",
      "1196 [D loss: 0.609079, acc.: 56.25%] [G loss: 0.856120]\n",
      "1197 [D loss: 0.599249, acc.: 65.62%] [G loss: 0.834749]\n",
      "1198 [D loss: 0.606605, acc.: 65.62%] [G loss: 0.842389]\n",
      "1199 [D loss: 0.617984, acc.: 65.62%] [G loss: 0.864746]\n",
      "1200 [D loss: 0.586006, acc.: 59.38%] [G loss: 0.817165]\n",
      "1201 [D loss: 0.614039, acc.: 62.50%] [G loss: 0.868947]\n",
      "1202 [D loss: 0.624099, acc.: 68.75%] [G loss: 0.849779]\n",
      "1203 [D loss: 0.595976, acc.: 78.12%] [G loss: 0.824742]\n",
      "1204 [D loss: 0.693194, acc.: 50.00%] [G loss: 0.844192]\n",
      "1205 [D loss: 0.668876, acc.: 56.25%] [G loss: 0.872307]\n",
      "1206 [D loss: 0.573728, acc.: 75.00%] [G loss: 0.866646]\n",
      "1207 [D loss: 0.625188, acc.: 68.75%] [G loss: 0.864094]\n",
      "1208 [D loss: 0.626523, acc.: 65.62%] [G loss: 0.860569]\n",
      "1209 [D loss: 0.603932, acc.: 71.88%] [G loss: 0.865585]\n",
      "1210 [D loss: 0.612099, acc.: 71.88%] [G loss: 0.909340]\n",
      "1211 [D loss: 0.668735, acc.: 56.25%] [G loss: 0.848381]\n",
      "1212 [D loss: 0.617965, acc.: 65.62%] [G loss: 0.881997]\n",
      "1213 [D loss: 0.577430, acc.: 68.75%] [G loss: 0.892991]\n",
      "1214 [D loss: 0.619849, acc.: 65.62%] [G loss: 0.797167]\n",
      "1215 [D loss: 0.673800, acc.: 56.25%] [G loss: 0.801166]\n",
      "1216 [D loss: 0.589244, acc.: 59.38%] [G loss: 0.865008]\n",
      "1217 [D loss: 0.651461, acc.: 62.50%] [G loss: 0.849628]\n",
      "1218 [D loss: 0.647084, acc.: 68.75%] [G loss: 0.835380]\n",
      "1219 [D loss: 0.623946, acc.: 71.88%] [G loss: 0.794461]\n",
      "1220 [D loss: 0.620407, acc.: 71.88%] [G loss: 0.885529]\n",
      "1221 [D loss: 0.699224, acc.: 46.88%] [G loss: 0.851235]\n",
      "1222 [D loss: 0.640320, acc.: 53.12%] [G loss: 0.802086]\n",
      "1223 [D loss: 0.631055, acc.: 56.25%] [G loss: 0.871481]\n",
      "1224 [D loss: 0.611476, acc.: 65.62%] [G loss: 0.851109]\n",
      "1225 [D loss: 0.676526, acc.: 53.12%] [G loss: 0.831086]\n",
      "1226 [D loss: 0.657284, acc.: 68.75%] [G loss: 0.827932]\n",
      "1227 [D loss: 0.687048, acc.: 59.38%] [G loss: 0.821164]\n",
      "1228 [D loss: 0.635799, acc.: 59.38%] [G loss: 0.779791]\n",
      "1229 [D loss: 0.589766, acc.: 59.38%] [G loss: 0.816924]\n",
      "1230 [D loss: 0.582805, acc.: 84.38%] [G loss: 0.826297]\n",
      "1231 [D loss: 0.640922, acc.: 65.62%] [G loss: 0.872182]\n",
      "1232 [D loss: 0.654325, acc.: 59.38%] [G loss: 0.857614]\n",
      "1233 [D loss: 0.590550, acc.: 68.75%] [G loss: 0.876255]\n",
      "1234 [D loss: 0.634667, acc.: 62.50%] [G loss: 0.865120]\n",
      "1235 [D loss: 0.609740, acc.: 62.50%] [G loss: 0.839943]\n",
      "1236 [D loss: 0.650513, acc.: 68.75%] [G loss: 0.823829]\n",
      "1237 [D loss: 0.593064, acc.: 75.00%] [G loss: 0.853935]\n",
      "1238 [D loss: 0.653435, acc.: 46.88%] [G loss: 0.817021]\n",
      "1239 [D loss: 0.576899, acc.: 68.75%] [G loss: 0.893312]\n",
      "1240 [D loss: 0.607848, acc.: 78.12%] [G loss: 0.845649]\n",
      "1241 [D loss: 0.673628, acc.: 53.12%] [G loss: 0.879457]\n",
      "1242 [D loss: 0.600161, acc.: 65.62%] [G loss: 0.842618]\n",
      "1243 [D loss: 0.733780, acc.: 43.75%] [G loss: 0.797440]\n",
      "1244 [D loss: 0.607245, acc.: 71.88%] [G loss: 0.803070]\n",
      "1245 [D loss: 0.640090, acc.: 62.50%] [G loss: 0.790318]\n",
      "1246 [D loss: 0.597865, acc.: 71.88%] [G loss: 0.852049]\n",
      "1247 [D loss: 0.596171, acc.: 75.00%] [G loss: 0.817662]\n",
      "1248 [D loss: 0.702978, acc.: 50.00%] [G loss: 0.867795]\n",
      "1249 [D loss: 0.676523, acc.: 56.25%] [G loss: 0.830132]\n",
      "1250 [D loss: 0.679028, acc.: 50.00%] [G loss: 0.822209]\n",
      "1251 [D loss: 0.648006, acc.: 62.50%] [G loss: 0.858660]\n",
      "1252 [D loss: 0.591394, acc.: 75.00%] [G loss: 0.859973]\n",
      "1253 [D loss: 0.605538, acc.: 71.88%] [G loss: 0.870759]\n",
      "1254 [D loss: 0.644451, acc.: 56.25%] [G loss: 0.824416]\n",
      "1255 [D loss: 0.617693, acc.: 68.75%] [G loss: 0.844189]\n",
      "1256 [D loss: 0.582268, acc.: 75.00%] [G loss: 0.825973]\n",
      "1257 [D loss: 0.636929, acc.: 59.38%] [G loss: 0.824162]\n",
      "1258 [D loss: 0.630660, acc.: 56.25%] [G loss: 0.862329]\n",
      "1259 [D loss: 0.567014, acc.: 81.25%] [G loss: 0.855151]\n",
      "1260 [D loss: 0.612670, acc.: 68.75%] [G loss: 0.822345]\n",
      "1261 [D loss: 0.665828, acc.: 56.25%] [G loss: 0.847809]\n",
      "1262 [D loss: 0.553714, acc.: 78.12%] [G loss: 0.881161]\n",
      "1263 [D loss: 0.623842, acc.: 75.00%] [G loss: 0.879811]\n",
      "1264 [D loss: 0.616841, acc.: 68.75%] [G loss: 0.812869]\n",
      "1265 [D loss: 0.637413, acc.: 56.25%] [G loss: 0.881385]\n",
      "1266 [D loss: 0.674110, acc.: 59.38%] [G loss: 0.815924]\n",
      "1267 [D loss: 0.627730, acc.: 68.75%] [G loss: 0.825819]\n",
      "1268 [D loss: 0.628032, acc.: 56.25%] [G loss: 0.863635]\n",
      "1269 [D loss: 0.601882, acc.: 71.88%] [G loss: 0.822963]\n",
      "1270 [D loss: 0.589362, acc.: 71.88%] [G loss: 0.858443]\n",
      "1271 [D loss: 0.670085, acc.: 46.88%] [G loss: 0.854280]\n",
      "1272 [D loss: 0.624334, acc.: 71.88%] [G loss: 0.842722]\n",
      "1273 [D loss: 0.587926, acc.: 68.75%] [G loss: 0.823229]\n",
      "1274 [D loss: 0.600886, acc.: 71.88%] [G loss: 0.799080]\n",
      "1275 [D loss: 0.672168, acc.: 53.12%] [G loss: 0.798945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1276 [D loss: 0.620156, acc.: 68.75%] [G loss: 0.822175]\n",
      "1277 [D loss: 0.570085, acc.: 84.38%] [G loss: 0.815933]\n",
      "1278 [D loss: 0.558651, acc.: 87.50%] [G loss: 0.798788]\n",
      "1279 [D loss: 0.579887, acc.: 78.12%] [G loss: 0.803300]\n",
      "1280 [D loss: 0.683201, acc.: 62.50%] [G loss: 0.804431]\n",
      "1281 [D loss: 0.619212, acc.: 59.38%] [G loss: 0.833463]\n",
      "1282 [D loss: 0.591724, acc.: 71.88%] [G loss: 0.828547]\n",
      "1283 [D loss: 0.617775, acc.: 59.38%] [G loss: 0.882289]\n",
      "1284 [D loss: 0.586461, acc.: 71.88%] [G loss: 0.867380]\n",
      "1285 [D loss: 0.569631, acc.: 84.38%] [G loss: 0.939691]\n",
      "1286 [D loss: 0.672464, acc.: 50.00%] [G loss: 0.852761]\n",
      "1287 [D loss: 0.575026, acc.: 71.88%] [G loss: 0.849691]\n",
      "1288 [D loss: 0.592373, acc.: 81.25%] [G loss: 0.855561]\n",
      "1289 [D loss: 0.595973, acc.: 75.00%] [G loss: 0.880761]\n",
      "1290 [D loss: 0.613256, acc.: 68.75%] [G loss: 0.840073]\n",
      "1291 [D loss: 0.604364, acc.: 71.88%] [G loss: 0.841256]\n",
      "1292 [D loss: 0.587749, acc.: 68.75%] [G loss: 0.841201]\n",
      "1293 [D loss: 0.566284, acc.: 78.12%] [G loss: 0.892696]\n",
      "1294 [D loss: 0.629229, acc.: 59.38%] [G loss: 0.811578]\n",
      "1295 [D loss: 0.587001, acc.: 65.62%] [G loss: 0.918112]\n",
      "1296 [D loss: 0.606326, acc.: 71.88%] [G loss: 0.888335]\n",
      "1297 [D loss: 0.606811, acc.: 68.75%] [G loss: 0.803638]\n",
      "1298 [D loss: 0.641671, acc.: 68.75%] [G loss: 0.859031]\n",
      "1299 [D loss: 0.623307, acc.: 71.88%] [G loss: 0.828070]\n",
      "1300 [D loss: 0.555715, acc.: 78.12%] [G loss: 0.882492]\n",
      "1301 [D loss: 0.684757, acc.: 53.12%] [G loss: 0.809741]\n",
      "1302 [D loss: 0.611966, acc.: 71.88%] [G loss: 0.840620]\n",
      "1303 [D loss: 0.627893, acc.: 68.75%] [G loss: 0.854418]\n",
      "1304 [D loss: 0.596870, acc.: 68.75%] [G loss: 0.866522]\n",
      "1305 [D loss: 0.652610, acc.: 62.50%] [G loss: 0.835117]\n",
      "1306 [D loss: 0.590254, acc.: 84.38%] [G loss: 0.831689]\n",
      "1307 [D loss: 0.615315, acc.: 56.25%] [G loss: 0.811407]\n",
      "1308 [D loss: 0.663085, acc.: 56.25%] [G loss: 0.792663]\n",
      "1309 [D loss: 0.513217, acc.: 87.50%] [G loss: 0.867833]\n",
      "1310 [D loss: 0.590595, acc.: 75.00%] [G loss: 0.892884]\n",
      "1311 [D loss: 0.651571, acc.: 59.38%] [G loss: 0.860696]\n",
      "1312 [D loss: 0.593291, acc.: 78.12%] [G loss: 0.897775]\n",
      "1313 [D loss: 0.549099, acc.: 75.00%] [G loss: 0.918657]\n",
      "1314 [D loss: 0.628906, acc.: 71.88%] [G loss: 0.854808]\n",
      "1315 [D loss: 0.650534, acc.: 59.38%] [G loss: 0.830711]\n",
      "1316 [D loss: 0.567023, acc.: 71.88%] [G loss: 0.871110]\n",
      "1317 [D loss: 0.625735, acc.: 65.62%] [G loss: 0.863601]\n",
      "1318 [D loss: 0.617833, acc.: 68.75%] [G loss: 0.913123]\n",
      "1319 [D loss: 0.637210, acc.: 65.62%] [G loss: 0.844211]\n",
      "1320 [D loss: 0.591984, acc.: 75.00%] [G loss: 0.821123]\n",
      "1321 [D loss: 0.636326, acc.: 68.75%] [G loss: 0.844026]\n",
      "1322 [D loss: 0.575637, acc.: 71.88%] [G loss: 0.841131]\n",
      "1323 [D loss: 0.588794, acc.: 62.50%] [G loss: 0.810954]\n",
      "1324 [D loss: 0.641591, acc.: 62.50%] [G loss: 0.857135]\n",
      "1325 [D loss: 0.591866, acc.: 68.75%] [G loss: 0.907995]\n",
      "1326 [D loss: 0.608493, acc.: 68.75%] [G loss: 0.840434]\n",
      "1327 [D loss: 0.624317, acc.: 71.88%] [G loss: 0.852034]\n",
      "1328 [D loss: 0.549416, acc.: 71.88%] [G loss: 0.851465]\n",
      "1329 [D loss: 0.598270, acc.: 68.75%] [G loss: 0.859056]\n",
      "1330 [D loss: 0.550137, acc.: 87.50%] [G loss: 0.811338]\n",
      "1331 [D loss: 0.648364, acc.: 65.62%] [G loss: 0.839491]\n",
      "1332 [D loss: 0.587518, acc.: 71.88%] [G loss: 0.959099]\n",
      "1333 [D loss: 0.578155, acc.: 71.88%] [G loss: 0.900579]\n",
      "1334 [D loss: 0.630398, acc.: 62.50%] [G loss: 0.818850]\n",
      "1335 [D loss: 0.568987, acc.: 81.25%] [G loss: 0.923152]\n",
      "1336 [D loss: 0.605189, acc.: 75.00%] [G loss: 0.971846]\n",
      "1337 [D loss: 0.565425, acc.: 78.12%] [G loss: 0.903782]\n",
      "1338 [D loss: 0.600050, acc.: 71.88%] [G loss: 0.846838]\n",
      "1339 [D loss: 0.647545, acc.: 68.75%] [G loss: 0.848901]\n",
      "1340 [D loss: 0.588014, acc.: 75.00%] [G loss: 0.901682]\n",
      "1341 [D loss: 0.617764, acc.: 62.50%] [G loss: 0.889380]\n",
      "1342 [D loss: 0.684063, acc.: 53.12%] [G loss: 0.838800]\n",
      "1343 [D loss: 0.559976, acc.: 78.12%] [G loss: 0.830536]\n",
      "1344 [D loss: 0.615506, acc.: 71.88%] [G loss: 0.870543]\n",
      "1345 [D loss: 0.582570, acc.: 68.75%] [G loss: 0.852019]\n",
      "1346 [D loss: 0.643036, acc.: 59.38%] [G loss: 0.857843]\n",
      "1347 [D loss: 0.588067, acc.: 75.00%] [G loss: 0.901337]\n",
      "1348 [D loss: 0.557347, acc.: 71.88%] [G loss: 0.992595]\n",
      "1349 [D loss: 0.596764, acc.: 75.00%] [G loss: 0.895928]\n",
      "1350 [D loss: 0.683453, acc.: 59.38%] [G loss: 0.880759]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-309c7699daee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0mgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m     \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-309c7699daee>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[1;31m# Train the generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;31m# Plot the progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1762\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1763\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1764\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28 \n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', \n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(100,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity \n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        noise_shape = (100,)\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_shape=noise_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=30000, batch_size=32, save_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
