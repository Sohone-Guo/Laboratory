<!DOCTYPE html>
<!-- saved from url=(0083)https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html -->
<html class="js flexbox canvas canvastext webgl no-touch geolocation postmessage websqldatabase indexeddb hashchange history draganddrop websockets rgba hsla multiplebgs backgroundsize borderimage borderradius boxshadow textshadow opacity cssanimations csscolumns cssgradients cssreflections csstransforms csstransforms3d csstransitions fontface generatedcontent video audio localstorage sessionstorage webworkers applicationcache svg inlinesvg smil svgclippaths gr__pytorch_org" lang="en" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta content="width=device-width, initial-scale=1.0" name="viewport">
<title>Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation</title>
<style class="anchorjs"></style><link href="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/theme.css" rel="stylesheet" type="text/css">
<!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
<link href="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/gallery.css" rel="stylesheet" type="text/css">
<link href="https://pytorch.org/tutorials/genindex.html" rel="index" title="Index">
<link href="https://pytorch.org/tutorials/search.html" rel="search" title="Search">
<link href="https://pytorch.org/tutorials/beginner/saving_loading_models.html" rel="next" title="Saving and Loading Models">
<link href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html" rel="prev" title="Transfer Learning Tutorial">
<script async="" src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/analytics.js.下载"></script><script src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/modernizr.min.js.下载"></script>
<style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.1') format('opentype')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>
<body class="pytorch-body" data-gr-c-s-loaded="true"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div><div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="container">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/features">Features</a>
</li>
<li>
<a href="https://pytorch.org/ecosystem">Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li class="active">
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/docs/stable/index.html">Docs</a>
</li>
<li>
<a href="https://pytorch.org/resources">Resources</a>
</li>
<li>
<a href="https://github.com/pytorch/pytorch">Github</a>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#"></a>
</div>
</div>
</div>

<div class="table-of-contents-link-wrapper">
<span>Table of Contents</span>
<a class="toggle-table-of-contents" data-behavior="toggle-table-of-contents" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#"></a>
</div>
<nav class="pytorch-left-menu" data-toggle="wy-nav-shift" id="pytorch-left-menu" style="height: 100%;">
<div class="pytorch-side-scroll">
<div aria-label="main navigation" class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation">
<div class="pytorch-left-menu-search">
<div class="version">
                  1.0.0.dev20181207
                </div>
<div role="search">
<form action="https://pytorch.org/tutorials/search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search Tutorials" type="text">
<input name="check_keywords" type="hidden" value="yes">
<input name="area" type="hidden" value="default">
</form>
</div>
</div>
<p class="caption"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">Deep Learning with PyTorch: A 60 Minute Blitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html">Data Loading and Processing Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html">Learning PyTorch with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html">Transfer Learning Tutorial</a></li>
<li class="toctree-l1 current"><a class="reference internal current" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#">Deploying a Seq2Seq Model with the Hybrid Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">Saving and Loading Models</a></li>
</ul>
<p class="caption"><span class="caption-text">Image</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html">Finetuning Torchvision Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html">Spatial Transformer Networks Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/advanced/neural_style_tutorial.html">Neural Transfer Using PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/beginner/fgsm_tutorial.html">Adversarial Example Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html">Transfering a Model from PyTorch to Caffe2 and Mobile using ONNX</a></li>
</ul>
<p class="caption"><span class="caption-text">Text</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/beginner/chatbot_tutorial.html">Chatbot Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html">Generating Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html">Classifying Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html">Deep Learning for NLP with Pytorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">Translation with a Sequence to Sequence Network and Attention</a></li>
</ul>
<p class="caption"><span class="caption-text">Generative</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html">DCGAN Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">Reinforcement Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html">Reinforcement Learning (DQN) Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">Extending PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html">Creating Extensions Using numpy and scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a></li>
</ul>
<p class="caption"><span class="caption-text">Production Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/intermediate/dist_tuto.html">Writing Distributed Applications with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/advanced/ONNXLive.html">ONNX Live Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/tutorials/advanced/cpp_export.html">Loading a PyTorch Model in C++</a></li>
</ul>
</div>
</div>
</nav>
<div class="pytorch-container">
<div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
<div class="pytorch-breadcrumbs-wrapper">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="pytorch-breadcrumbs">
<li>
<a href="https://pytorch.org/tutorials/index.html">
          
            Tutorials
          
        </a> &gt;
      </li>
<li>Deploying a Seq2Seq Model with the Hybrid Frontend</li>
<li class="pytorch-breadcrumbs-aside">
<a href="https://pytorch.org/tutorials/_sources/beginner/deploy_seq2seq_hybrid_frontend_tutorial.rst.txt" rel="nofollow"><img src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/view-page-source-icon.svg"></a>
</li>
</ul>
</div>
</div>
<div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper" style="display: block;">
          Shortcuts
        </div>
</div>
<section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
<div class="pytorch-content-left">
<div class="rst-content">
<div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
<div class="sphx-glr-download-link-note admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Click <a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#sphx-glr-download-beginner-deploy-seq2seq-hybrid-frontend-tutorial-py"><span class="std std-ref">here</span></a> to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="deploying-a-seq2seq-model-with-the-hybrid-frontend">
<span id="sphx-glr-beginner-deploy-seq2seq-hybrid-frontend-tutorial-py"></span><h1>Deploying a Seq2Seq Model with the Hybrid Frontend<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#deploying-a-seq2seq-model-with-the-hybrid-frontend" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h1>
<p><strong>Author:</strong> <a class="reference external" href="https://github.com/MatthewInkawhich">Matthew Inkawhich</a></p>
<p>This tutorial will walk through the process of transitioning a
sequence-to-sequence model to Torch Script using PyTorch’s Hybrid
Frontend. The model that we will convert is the chatbot model from the
<a class="reference external" href="https://pytorch.org/tutorials/beginner/chatbot_tutorial.html">Chatbot tutorial</a>.
You can either treat this tutorial as a “Part 2” to the Chatbot tutorial
and deploy your own pretrained model, or you can start with this
document and use a pretrained model that we host. In the latter case,
you can reference the original Chatbot tutorial for details
regarding data preprocessing, model theory and definition, and model
training.</p>
<div class="section" id="what-is-the-hybrid-frontend">
<h2>What is the Hybrid Frontend?<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#what-is-the-hybrid-frontend" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<p>During the research and development phase of a deep learning-based
project, it is advantageous to interact with an <strong>eager</strong>, imperative
interface like PyTorch’s. This gives users the ability to write
familiar, idiomatic Python, allowing for the use of Python data
structures, control flow operations, print statements, and debugging
utilities. Although the eager interface is a beneficial tool for
research and experimentation applications, when it comes time to deploy
the model in a production environment, having a <strong>graph</strong>-based model
representation is very beneficial. A deferred graph representation
allows for optimizations such as out-of-order execution, and the ability
to target highly optimized hardware architectures. Also, a graph-based
representation enables framework-agnostic model exportation. PyTorch
provides mechanisms for incrementally converting eager-mode code into
Torch Script, a statically analyzable and optimizable subset of Python
that Torch uses to represent deep learning programs independently from
the Python runtime.</p>
<p>The API for converting eager-mode PyTorch programs into Torch Script is
found in the torch.jit module. This module has two core modalities for
converting an eager-mode model to a Torch Script graph representation:
<strong>tracing</strong> and <strong>scripting</strong>. The <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code> function takes a
module or function and a set of example inputs. It then runs the example
input through the function or module while tracing the computational
steps that are encountered, and outputs a graph-based function that
performs the traced operations. <strong>Tracing</strong> is great for straightforward
modules and functions that do not involve data-dependent control flow,
such as standard convolutional neural networks. However, if a function
with data-dependent if statements and loops is traced, only the
operations called along the execution route taken by the example input
will be recorded. In other words, the control flow itself is not
captured. To convert modules and functions containing data-dependent
control flow, a <strong>scripting</strong> mechanism is provided. Scripting
explicitly converts the module or function code to Torch Script,
including all possible control flow routes. To use script mode, be sure
to inherit from the the <code class="docutils literal notranslate"><span class="pre">torch.jit.ScriptModule</span></code> base class (instead
of <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) and add a <code class="docutils literal notranslate"><span class="pre">torch.jit.script</span></code> decorator to your
Python function or a <code class="docutils literal notranslate"><span class="pre">torch.jit.script_method</span></code> decorator to your
module’s methods. The one caveat with using scripting is that it only
supports a restricted subset of Python. For all details relating to the
supported features, see the Torch Script <a class="reference external" href="https://pytorch.org/docs/master/jit.html">language
reference</a>. To provide the
maximum flexibility, the modes of Torch Script can be composed to
represent your whole program, and these techniques can be applied
incrementally.</p>
<div class="figure align-center">
<img alt="workflow" src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/pytorch_workflow.png">
</div>
</div>
<div class="section" id="acknowledgements">
<h2>Acknowledgements<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#acknowledgements" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<p>This tutorial was inspired by the following sources:</p>
<ol class="arabic simple">
<li>Yuan-Kuei Wu’s pytorch-chatbot implementation:
<a class="reference external" href="https://github.com/ywk991112/pytorch-chatbot">https://github.com/ywk991112/pytorch-chatbot</a></li>
<li>Sean Robertson’s practical-pytorch seq2seq-translation example:
<a class="reference external" href="https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation">https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation</a></li>
<li>FloydHub’s Cornell Movie Corpus preprocessing code:
<a class="reference external" href="https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus">https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus</a></li>
</ol>
</div>
<div class="section" id="prepare-environment">
<h2>Prepare Environment<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#prepare-environment" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<p>First, we will import the required modules and set some constants. If
you are planning on using your own model, be sure that the
<code class="docutils literal notranslate"><span class="pre">MAX_LENGTH</span></code> constant is set correctly. As a reminder, this constant
defines the maximum allowed sentence length during training and the
maximum length output that the model is capable of producing.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">unicode_literals</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>


<span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Maximum sentence length</span>

<span class="c1"># Default word tokens</span>
<span class="n">PAD_token</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Used for padding short sentences</span>
<span class="n">SOS_token</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Start-of-sentence token</span>
<span class="n">EOS_token</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># End-of-sentence token</span>
</pre></div>
</div>
</div>
<div class="section" id="model-overview">
<h2>Model Overview<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#model-overview" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<p>As mentioned, the model that we are using is a
<a class="reference external" href="https://arxiv.org/abs/1409.3215">sequence-to-sequence</a> (seq2seq)
model. This type of model is used in cases when our input is a
variable-length sequence, and our output is also a variable length
sequence that is not necessarily a one-to-one mapping of the input. A
seq2seq model is comprised of two recurrent neural networks (RNNs) that
work cooperatively: an <strong>encoder</strong> and a <strong>decoder</strong>.</p>
<div class="figure align-center">
<img alt="model" src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/seq2seq_ts.png">
</div>
<p>Image source:
<a class="reference external" href="https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/">https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/</a></p>
<div class="section" id="encoder">
<h3>Encoder<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#encoder" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h3>
<p>The encoder RNN iterates through the input sentence one token
(e.g.&nbsp;word) at a time, at each time step outputting an “output” vector
and a “hidden state” vector. The hidden state vector is then passed to
the next time step, while the output vector is recorded. The encoder
transforms the context it saw at each point in the sequence into a set
of points in a high-dimensional space, which the decoder will use to
generate a meaningful output for the given task.</p>
</div>
<div class="section" id="decoder">
<h3>Decoder<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#decoder" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h3>
<p>The decoder RNN generates the response sentence in a token-by-token
fashion. It uses the encoder’s context vectors, and internal hidden
states to generate the next word in the sequence. It continues
generating words until it outputs an <em>EOS_token</em>, representing the end
of the sentence. We use an <a class="reference external" href="https://arxiv.org/abs/1409.0473">attention
mechanism</a> in our decoder to help it
to “pay attention” to certain parts of the input when generating the
output. For our model, we implement <a class="reference external" href="https://arxiv.org/abs/1508.04025">Luong et
al.</a>’s “Global attention” module,
and use it as a submodule in our decode model.</p>
</div>
</div>
<div class="section" id="data-handling">
<h2>Data Handling<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#data-handling" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<p>Although our models conceptually deal with sequences of tokens, in
reality, they deal with numbers like all machine learning models do. In
this case, every word in the model’s vocabulary, which was established
before training, is mapped to an integer index. We use a <code class="docutils literal notranslate"><span class="pre">Voc</span></code> object
to contain the mappings from word to index, as well as the total number
of words in the vocabulary. We will load the object later before we run
the model.</p>
<p>Also, in order for us to be able to run evaluations, we must provide a
tool for processing our string inputs. The <code class="docutils literal notranslate"><span class="pre">normalizeString</span></code> function
converts all characters in a string to lowercase and removes all
non-letter characters. The <code class="docutils literal notranslate"><span class="pre">indexesFromSentence</span></code> function takes a
sentence of words and returns the corresponding sequence of word
indexes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Voc</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD_token</span><span class="p">:</span> <span class="s2">"PAD"</span><span class="p">,</span> <span class="n">SOS_token</span><span class="p">:</span> <span class="s2">"SOS"</span><span class="p">,</span> <span class="n">EOS_token</span><span class="p">:</span> <span class="s2">"EOS"</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Count SOS, EOS, PAD</span>

    <span class="k">def</span> <span class="nf">addSentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">addWord</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">addWord</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_words</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Remove words below a certain count threshold</span>
    <span class="k">def</span> <span class="nf">trim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_count</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">keep_words</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">v</span> <span class="o">&gt;=</span> <span class="n">min_count</span><span class="p">:</span>
                <span class="n">keep_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="s1">'keep_words {} / {} = {:.4f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">keep_words</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_words</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">)</span>
        <span class="p">))</span>
        <span class="c1"># Reinitialize dictionaries</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD_token</span><span class="p">:</span> <span class="s2">"PAD"</span><span class="p">,</span> <span class="n">SOS_token</span><span class="p">:</span> <span class="s2">"SOS"</span><span class="p">,</span> <span class="n">EOS_token</span><span class="p">:</span> <span class="s2">"EOS"</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># Count default tokens</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">keep_words</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">addWord</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>


<span class="c1"># Lowercase and remove non-letter characters</span>
<span class="k">def</span> <span class="nf">normalizeString</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"([.!?])"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">" \1"</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"[^a-zA-Z.!?]+"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">" "</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">s</span>


<span class="c1"># Takes string sentence, returns sentence of word indexes</span>
<span class="k">def</span> <span class="nf">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">voc</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">EOS_token</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="define-encoder">
<h2>Define Encoder<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#define-encoder" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<p>We implement our encoder’s RNN with the <code class="docutils literal notranslate"><span class="pre">torch.nn.GRU</span></code> module which we
feed a batch of sentences (vectors of word embeddings) and it internally
iterates through the sentences one token at a time calculating the
hidden states. We initialize this module to be bidirectional, meaning
that we have two independent GRUs: one that iterates through the
sequences in chronological order, and another that iterates in reverse
order. We ultimately return the sum of these two GRUs’ outputs. Since
our model was trained using batching, our <code class="docutils literal notranslate"><span class="pre">EncoderRNN</span></code> model’s
<code class="docutils literal notranslate"><span class="pre">forward</span></code> function expects a padded input batch. To batch
variable-length sentences, we allow a maximum of <em>MAX_LENGTH</em> tokens in
a sentence, and all sentences in the batch that have less than
<em>MAX_LENGTH</em> tokens are padded at the end with our dedicated <em>PAD_token</em>
tokens. To use padded batches with a PyTorch RNN module, we must wrap
the forward pass call with <code class="docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence</span></code>
and <code class="docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pad_packed_sequence</span></code> data transformations.
Note that the <code class="docutils literal notranslate"><span class="pre">forward</span></code> function also takes an <code class="docutils literal notranslate"><span class="pre">input_lengths</span></code> list,
which contains the length of each sentence in the batch. This input is
used by the <code class="docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence</span></code> function when
padding.</p>
<div class="section" id="hybrid-frontend-notes">
<h3>Hybrid Frontend Notes:<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#hybrid-frontend-notes" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h3>
<p>Since the encoder’s <code class="docutils literal notranslate"><span class="pre">forward</span></code> function does not contain any
data-dependent control flow, we will use <strong>tracing</strong> to convert it to
script mode. When tracing a module, we can leave the module definition
as-is. We will initialize all models towards the end of this document
before we run evaluations.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EncoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span>

        <span class="c1"># Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'</span>
        <span class="c1">#   because our input size is a word embedding with number of features == hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span>
                          <span class="n">dropout</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">n_layers</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">dropout</span><span class="p">),</span> <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Convert word indexes to embeddings</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span>
        <span class="c1"># Pack padded batch of sequences for RNN module</span>
        <span class="n">packed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pack_padded_sequence</span><span class="p">(</span><span class="n">embedded</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">)</span>
        <span class="c1"># Forward pass through GRU</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">packed</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="c1"># Unpack padding</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_packed_sequence</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="c1"># Sum bidirectional GRU outputs</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[:,</span> <span class="p">:</span> <span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">:]</span>
        <span class="c1"># Return output and final hidden state</span>
        <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="define-decoders-attention-module">
<h2>Define Decoder’s Attention Module<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#define-decoders-attention-module" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<p>Next, we’ll define our attention module (<code class="docutils literal notranslate"><span class="pre">Attn</span></code>). Note that this
module will be used as a submodule in our decoder model. Luong et
al.&nbsp;consider various “score functions”, which take the current decoder
RNN output and the entire encoder output, and return attention
“energies”. This attention energies tensor is the same size as the
encoder output, and the two are ultimately multiplied, resulting in a
weighted tensor whose largest values represent the most important parts
of the query sentence at a particular time-step of decoding.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Luong attention layer</span>
<span class="k">class</span> <span class="nc">Attn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Attn</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">'dot'</span><span class="p">,</span> <span class="s1">'general'</span><span class="p">,</span> <span class="s1">'concat'</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">,</span> <span class="s2">"is not an appropriate attention method."</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">'general'</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">'concat'</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">dot_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">hidden</span> <span class="o">*</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">general_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="n">energy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">hidden</span> <span class="o">*</span> <span class="n">energy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">concat_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="n">energy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">hidden</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">encoder_output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">encoder_output</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">*</span> <span class="n">energy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">):</span>
        <span class="c1"># Calculate the attention weights (energies) based on the given method</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">'general'</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">general_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">'concat'</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">'dot'</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dot_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>

        <span class="c1"># Transpose max_length and batch_size dimensions</span>
        <span class="n">attn_energies</span> <span class="o">=</span> <span class="n">attn_energies</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

        <span class="c1"># Return the softmax normalized probability scores (with added dimension)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_energies</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="define-decoder">
<h2>Define Decoder<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#define-decoder" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<p>Similarly to the <code class="docutils literal notranslate"><span class="pre">EncoderRNN</span></code>, we use the <code class="docutils literal notranslate"><span class="pre">torch.nn.GRU</span></code> module for
our decoder’s RNN. This time, however, we use a unidirectional GRU. It
is important to note that unlike the encoder, we will feed the decoder
RNN one word at a time. We start by getting the embedding of the current
word and applying a
<a class="reference external" href="https://pytorch.org/docs/stable/nn.html?highlight=dropout#torch.nn.Dropout">dropout</a>.
Next, we forward the embedding and the last hidden state to the GRU and
obtain a current GRU output and hidden state. We then use our <code class="docutils literal notranslate"><span class="pre">Attn</span></code>
module as a layer to obtain the attention weights, which we multiply by
the encoder’s output to obtain our attended encoder output. We use this
attended encoder output as our <code class="docutils literal notranslate"><span class="pre">context</span></code> tensor, which represents a
weighted sum indicating what parts of the encoder’s output to pay
attention to. From here, we use a linear layer and softmax normalization
to select the next word in the output sequence.</p>
<div class="section" id="id1">
<h3>Hybrid Frontend Notes:<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#id1" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h3>
<p>Similarly to the <code class="docutils literal notranslate"><span class="pre">EncoderRNN</span></code>, this module does not contain any
data-dependent control flow. Therefore, we can once again use
<strong>tracing</strong> to convert this model to Torch Script after it is
initialized and its parameters are loaded.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LuongAttnDecoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attn_model</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LuongAttnDecoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Keep for reference</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_model</span> <span class="o">=</span> <span class="n">attn_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

        <span class="c1"># Define layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">n_layers</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">dropout</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">Attn</span><span class="p">(</span><span class="n">attn_model</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_step</span><span class="p">,</span> <span class="n">last_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">):</span>
        <span class="c1"># Note: we run this one step (word) at a time</span>
        <span class="c1"># Get embedding of current input word</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">input_step</span><span class="p">)</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>
        <span class="c1"># Forward through unidirectional GRU</span>
        <span class="n">rnn_output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">embedded</span><span class="p">,</span> <span class="n">last_hidden</span><span class="p">)</span>
        <span class="c1"># Calculate attention weights from the current GRU output</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="c1"># Multiply attention weights to encoder outputs to get new "weighted sum" context vector</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="c1"># Concatenate weighted context vector and GRU output using Luong eq. 5</span>
        <span class="n">rnn_output</span> <span class="o">=</span> <span class="n">rnn_output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">concat_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">context</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">concat_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">concat_input</span><span class="p">))</span>
        <span class="c1"># Predict next word using Luong eq. 6</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">concat_output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Return output and final hidden state</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="define-evaluation">
<h2>Define Evaluation<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#define-evaluation" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<div class="section" id="greedy-search-decoder">
<h3>Greedy Search Decoder<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#greedy-search-decoder" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h3>
<p>As in the chatbot tutorial, we use a <code class="docutils literal notranslate"><span class="pre">GreedySearchDecoder</span></code> module to
facilitate the actual decoding process. This module has the trained
encoder and decoder models as attributes, and drives the process of
encoding an input sentence (a vector of word indexes), and iteratively
decoding an output response sequence one word (word index) at a time.</p>
<p>Encoding the input sequence is straightforward: simply forward the
entire sequence tensor and its corresponding lengths vector to the
<code class="docutils literal notranslate"><span class="pre">encoder</span></code>. It is important to note that this module only deals with
one input sequence at a time, <strong>NOT</strong> batches of sequences. Therefore,
when the constant <strong>1</strong> is used for declaring tensor sizes, this
corresponds to a batch size of 1. To decode a given decoder output, we
must iteratively run forward passes through our decoder model, which
outputs softmax scores corresponding to the probability of each word
being the correct next word in the decoded sequence. We initialize the
<code class="docutils literal notranslate"><span class="pre">decoder_input</span></code> to a tensor containing an <em>SOS_token</em>. After each pass
through the <code class="docutils literal notranslate"><span class="pre">decoder</span></code>, we <em>greedily</em> append the word with the highest
softmax probability to the <code class="docutils literal notranslate"><span class="pre">decoded_words</span></code> list. We also use this word
as the <code class="docutils literal notranslate"><span class="pre">decoder_input</span></code> for the next iteration. The decoding process
terminates either if the <code class="docutils literal notranslate"><span class="pre">decoded_words</span></code> list has reached a length of
<em>MAX_LENGTH</em> or if the predicted word is the <em>EOS_token</em>.</p>
</div>
<div class="section" id="id2">
<h3>Hybrid Frontend Notes:<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#id2" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">forward</span></code> method of this module involves iterating over the range
of <span class="math notranslate nohighlight"><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x005F;&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 3.612em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.857em; height: 0px; font-size: 52%;"><span style="position: absolute; clip: rect(1.088em, 1006.74em, 2.77em, -999.994em); top: -2.157em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mo" id="MathJax-Span-3" style="font-family: MathJax_Main;">[</span><span class="mn" id="MathJax-Span-4" style="font-family: MathJax_Main;">0</span><span class="mo" id="MathJax-Span-5" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-6" style="font-family: MathJax_Math-italic; padding-left: 0.126em;">m</span><span class="mi" id="MathJax-Span-7" style="font-family: MathJax_Math-italic;">a</span><span class="mi" id="MathJax-Span-8" style="font-family: MathJax_Math-italic;">x</span><span class="mi" id="MathJax-Span-9" style="font-family: MathJax_Main;">_</span><span class="mi" id="MathJax-Span-10" style="font-family: MathJax_Math-italic;">l</span><span class="mi" id="MathJax-Span-11" style="font-family: MathJax_Math-italic;">e</span><span class="mi" id="MathJax-Span-12" style="font-family: MathJax_Math-italic;">n</span><span class="mi" id="MathJax-Span-13" style="font-family: MathJax_Math-italic;">g<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.006em;"></span></span><span class="mi" id="MathJax-Span-14" style="font-family: MathJax_Math-italic;">t</span><span class="mi" id="MathJax-Span-15" style="font-family: MathJax_Math-italic;">h</span><span class="mo" id="MathJax-Span-16" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.169em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>m</mi><mi>a</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-1">[0, max\_length)</script></span> when decoding an output sequence one word at
a time. Because of this, we should use <strong>scripting</strong> to convert this
module to Torch Script. Unlike with our encoder and decoder models,
which we can trace, we must make some necessary changes to the
<code class="docutils literal notranslate"><span class="pre">GreedySearchDecoder</span></code> module in order to initialize an object without
error. In other words, we must ensure that our module adheres to the
rules of the scripting mechanism, and does not utilize any language
features outside of the subset of Python that Torch Script includes.</p>
<p>To get an idea of some manipulations that may be required, we will go
over the diffs between the <code class="docutils literal notranslate"><span class="pre">GreedySearchDecoder</span></code> implementation from
the chatbot tutorial and the implementation that we use in the cell
below. Note that the lines highlighted in red are lines removed from the
original implementation and the lines highlighted in green are new.</p>
<div class="figure align-center">
<img alt="diff" src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/diff.png">
</div>
<div class="section" id="changes">
<h4>Changes:<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#changes" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h4>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">torch.jit.ScriptModule</span></code><ul>
<li>In order to use PyTorch’s scripting mechanism on a module, that
module must inherit from the <code class="docutils literal notranslate"><span class="pre">torch.jit.ScriptModule</span></code>.</li>
</ul>
</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">decoder_n_layers</span></code> to the constructor arguments<ul>
<li>This change stems from the fact that the encoder and decoder
models that we pass to this module will be a child of
<code class="docutils literal notranslate"><span class="pre">TracedModule</span></code> (not <code class="docutils literal notranslate"><span class="pre">Module</span></code>). Therefore, we cannot access the
decoder’s number of layers with <code class="docutils literal notranslate"><span class="pre">decoder.n_layers</span></code>. Instead, we
plan for this, and pass this value in during module construction.</li>
</ul>
</li>
<li>Store away new attributes as constants<ul>
<li>In the original implementation, we were free to use variables from
the surrounding (global) scope in our <code class="docutils literal notranslate"><span class="pre">GreedySearchDecoder</span></code>’s
<code class="docutils literal notranslate"><span class="pre">forward</span></code> method. However, now that we are using scripting, we
do not have this freedom, as the assumption with scripting is that
we cannot necessarily hold on to Python objects, especially when
exporting. An easy solution to this is to store these values from
the global scope as attributes to the module in the constructor,
and add them to a special list called <code class="docutils literal notranslate"><span class="pre">__constants__</span></code> so that
they can be used as literal values when constructing the graph in
the <code class="docutils literal notranslate"><span class="pre">forward</span></code> method. An example of this usage is on NEW line
19, where instead of using the <code class="docutils literal notranslate"><span class="pre">device</span></code> and <code class="docutils literal notranslate"><span class="pre">SOS_token</span></code> global
values, we use our constant attributes <code class="docutils literal notranslate"><span class="pre">self._device</span></code> and
<code class="docutils literal notranslate"><span class="pre">self._SOS_token</span></code>.</li>
</ul>
</li>
<li>Add the <code class="docutils literal notranslate"><span class="pre">torch.jit.script_method</span></code> decorator to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>
method<ul>
<li>Adding this decorator lets the JIT compiler know that the function
that it is decorating should be scripted.</li>
</ul>
</li>
<li>Enforce types of <code class="docutils literal notranslate"><span class="pre">forward</span></code> method arguments<ul>
<li>By default, all parameters to a Torch Script function are assumed
to be Tensor. If we need to pass an argument of a different type,
we can use function type annotations as introduced in <a class="reference external" href="https://www.python.org/dev/peps/pep-3107/">PEP
3107</a>. In addition,
it is possible to declare arguments of different types using
MyPy-style type annotations (see
<a class="reference external" href="https://pytorch.org/docs/master/jit.html#types">doc</a>).</li>
</ul>
</li>
<li>Change initialization of <code class="docutils literal notranslate"><span class="pre">decoder_input</span></code><ul>
<li>In the original implementation, we initialized our
<code class="docutils literal notranslate"><span class="pre">decoder_input</span></code> tensor with <code class="docutils literal notranslate"><span class="pre">torch.LongTensor([[SOS_token]])</span></code>.
When scripting, we are not allowed to initialize tensors in a
literal fashion like this. Instead, we can initialize our tensor
with an explicit torch function such as <code class="docutils literal notranslate"><span class="pre">torch.ones</span></code>. In this
case, we can easily replicate the scalar <code class="docutils literal notranslate"><span class="pre">decoder_input</span></code> tensor
by multiplying 1 by our SOS_token value stored in the constant
<code class="docutils literal notranslate"><span class="pre">self._SOS_token</span></code>.</li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GreedySearchDecoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GreedySearchDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_SOS_token</span> <span class="o">=</span> <span class="n">SOS_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_decoder_n_layers</span> <span class="o">=</span> <span class="n">decoder_n_layers</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'_device'</span><span class="p">,</span> <span class="s1">'_SOS_token'</span><span class="p">,</span> <span class="s1">'_decoder_n_layers'</span><span class="p">]</span>

    <span class="nd">@torch.jit.script_method</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_seq</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">input_length</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">max_length</span> <span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="c1"># Forward input through encoder model</span>
        <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">input_length</span><span class="p">)</span>
        <span class="c1"># Prepare encoder's final hidden layer to be first hidden input to the decoder</span>
        <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">_decoder_n_layers</span><span class="p">]</span>
        <span class="c1"># Initialize decoder input with SOS_token</span>
        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_SOS_token</span>
        <span class="c1"># Initialize tensors to append decoded words to</span>
        <span class="n">all_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
        <span class="n">all_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="c1"># Iteratively decode one word token at a time</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>
            <span class="c1"># Forward pass through decoder</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
            <span class="c1"># Obtain most likely word token and its softmax score</span>
            <span class="n">decoder_scores</span><span class="p">,</span> <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Record token and score</span>
            <span class="n">all_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">all_tokens</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">all_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">all_scores</span><span class="p">,</span> <span class="n">decoder_scores</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># Prepare current token to be next decoder input (add a dimension)</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># Return collections of word tokens and scores</span>
        <span class="k">return</span> <span class="n">all_tokens</span><span class="p">,</span> <span class="n">all_scores</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="evaluating-an-input">
<h3>Evaluating an Input<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#evaluating-an-input" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h3>
<p>Next, we define some functions for evaluating an input. The <code class="docutils literal notranslate"><span class="pre">evaluate</span></code>
function takes a normalized string sentence, processes it to a tensor of
its corresponding word indexes (with batch size of 1), and passes this
tensor to a <code class="docutils literal notranslate"><span class="pre">GreedySearchDecoder</span></code> instance called <code class="docutils literal notranslate"><span class="pre">searcher</span></code> to
handle the encoding/decoding process. The searcher returns the output
word index vector and a scores tensor corresponding to the softmax
scores for each decoded word token. The final step is to convert each
word index back to its string representation using <code class="docutils literal notranslate"><span class="pre">voc.index2word</span></code>.</p>
<p>We also define two functions for evaluating an input sentence. The
<code class="docutils literal notranslate"><span class="pre">evaluateInput</span></code> function prompts a user for an input, and evaluates
it. It will continue to ask for another input until the user enters ‘q’
or ‘quit’.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">evaluateExample</span></code> function simply takes a string input sentence as
an argument, normalizes it, evaluates it, and prints the response.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
    <span class="c1">### Format input sentence as a batch</span>
    <span class="c1"># words -&gt; indexes</span>
    <span class="n">indexes_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)]</span>
    <span class="c1"># Create lengths tensor</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span> <span class="k">for</span> <span class="n">indexes</span> <span class="ow">in</span> <span class="n">indexes_batch</span><span class="p">])</span>
    <span class="c1"># Transpose dimensions of batch to match models' expectations</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">indexes_batch</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Use appropriate device</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Decode sentence with searcher</span>
    <span class="n">tokens</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">searcher</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
    <span class="c1"># indexes -&gt; words</span>
    <span class="n">decoded_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">voc</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">decoded_words</span>


<span class="c1"># Evaluate inputs from user input (stdin)</span>
<span class="k">def</span> <span class="nf">evaluateInput</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">):</span>
    <span class="n">input_sentence</span> <span class="o">=</span> <span class="s1">''</span>
    <span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Get input sentence</span>
            <span class="n">input_sentence</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s1">'&gt; '</span><span class="p">)</span>
            <span class="c1"># Check if it is quit case</span>
            <span class="k">if</span> <span class="n">input_sentence</span> <span class="o">==</span> <span class="s1">'q'</span> <span class="ow">or</span> <span class="n">input_sentence</span> <span class="o">==</span> <span class="s1">'quit'</span><span class="p">:</span> <span class="k">break</span>
            <span class="c1"># Normalize sentence</span>
            <span class="n">input_sentence</span> <span class="o">=</span> <span class="n">normalizeString</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">)</span>
            <span class="c1"># Evaluate sentence</span>
            <span class="n">output_words</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">input_sentence</span><span class="p">)</span>
            <span class="c1"># Format and print response sentence</span>
            <span class="n">output_words</span><span class="p">[:]</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output_words</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="s1">'EOS'</span> <span class="ow">or</span> <span class="n">x</span> <span class="o">==</span> <span class="s1">'PAD'</span><span class="p">)]</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">'Bot:'</span><span class="p">,</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_words</span><span class="p">))</span>

        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">"Error: Encountered unknown word."</span><span class="p">)</span>

<span class="c1"># Normalize input sentence and call evaluate()</span>
<span class="k">def</span> <span class="nf">evaluateExample</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"&gt; "</span> <span class="o">+</span> <span class="n">sentence</span><span class="p">)</span>
    <span class="c1"># Normalize sentence</span>
    <span class="n">input_sentence</span> <span class="o">=</span> <span class="n">normalizeString</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
    <span class="c1"># Evaluate sentence</span>
    <span class="n">output_words</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">input_sentence</span><span class="p">)</span>
    <span class="n">output_words</span><span class="p">[:]</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output_words</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="s1">'EOS'</span> <span class="ow">or</span> <span class="n">x</span> <span class="o">==</span> <span class="s1">'PAD'</span><span class="p">)]</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'Bot:'</span><span class="p">,</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_words</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="load-pretrained-parameters">
<h2>Load Pretrained Parameters<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#load-pretrained-parameters" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<p>Ok, its time to load our model!</p>
<div class="section" id="use-hosted-model">
<h3>Use hosted model<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#use-hosted-model" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h3>
<p>To load the hosted model:</p>
<ol class="arabic simple">
<li>Download the model <a class="reference external" href="https://download.pytorch.org/models/tutorials/4000_checkpoint.tar">here</a>.</li>
<li>Set the <code class="docutils literal notranslate"><span class="pre">loadFilename</span></code> variable to the path to the downloaded
checkpoint file.</li>
<li>Leave the <code class="docutils literal notranslate"><span class="pre">checkpoint</span> <span class="pre">=</span> <span class="pre">torch.load(loadFilename)</span></code> line uncommented,
as the hosted model was trained on CPU.</li>
</ol>
</div>
<div class="section" id="use-your-own-model">
<h3>Use your own model<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#use-your-own-model" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h3>
<p>To load your own pre-trained model:</p>
<ol class="arabic simple">
<li>Set the <code class="docutils literal notranslate"><span class="pre">loadFilename</span></code> variable to the path to the checkpoint file
that you wish to load. Note that if you followed the convention for
saving the model from the chatbot tutorial, this may involve changing
the <code class="docutils literal notranslate"><span class="pre">model_name</span></code>, <code class="docutils literal notranslate"><span class="pre">encoder_n_layers</span></code>, <code class="docutils literal notranslate"><span class="pre">decoder_n_layers</span></code>,
<code class="docutils literal notranslate"><span class="pre">hidden_size</span></code>, and <code class="docutils literal notranslate"><span class="pre">checkpoint_iter</span></code> (as these values are used in
the model path).</li>
<li>If you trained the model on a CPU, make sure that you are opening the
checkpoint with the <code class="docutils literal notranslate"><span class="pre">checkpoint</span> <span class="pre">=</span> <span class="pre">torch.load(loadFilename)</span></code> line.
If you trained the model on a GPU and are running this tutorial on a
CPU, uncomment the
<code class="docutils literal notranslate"><span class="pre">checkpoint</span> <span class="pre">=</span> <span class="pre">torch.load(loadFilename,</span> <span class="pre">map_location=torch.device('cpu'))</span></code>
line.</li>
</ol>
</div>
<div class="section" id="id3">
<h3>Hybrid Frontend Notes:<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#id3" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h3>
<p>Notice that we initialize and load parameters into our encoder and
decoder models as usual. Also, we must call <code class="docutils literal notranslate"><span class="pre">.to(device)</span></code> to set the
device options of the models and <code class="docutils literal notranslate"><span class="pre">.eval()</span></code> to set the dropout layers
to test mode <strong>before</strong> we trace the models. <code class="docutils literal notranslate"><span class="pre">TracedModule</span></code> objects do
not inherit the <code class="docutils literal notranslate"><span class="pre">to</span></code> or <code class="docutils literal notranslate"><span class="pre">eval</span></code> methods.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">save_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">"data"</span><span class="p">,</span> <span class="s2">"save"</span><span class="p">)</span>
<span class="n">corpus_name</span> <span class="o">=</span> <span class="s2">"cornell movie-dialogs corpus"</span>

<span class="c1"># Configure models</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s1">'cb_model'</span>
<span class="n">attn_model</span> <span class="o">=</span> <span class="s1">'dot'</span>
<span class="c1">#attn_model = 'general'</span>
<span class="c1">#attn_model = 'concat'</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">encoder_n_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">decoder_n_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># If you're loading your own model</span>
<span class="c1"># Set checkpoint to load from</span>
<span class="n">checkpoint_iter</span> <span class="o">=</span> <span class="mi">4000</span>
<span class="c1"># loadFilename = os.path.join(save_dir, model_name, corpus_name,</span>
<span class="c1">#                             '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),</span>
<span class="c1">#                             '{}_checkpoint.tar'.format(checkpoint_iter))</span>

<span class="c1"># If you're loading the hosted model</span>
<span class="n">loadFilename</span> <span class="o">=</span> <span class="s1">'data/4000_checkpoint.tar'</span>

<span class="c1"># Load model</span>
<span class="c1"># Force CPU device options (to match tensors in this tutorial)</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">loadFilename</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cpu'</span><span class="p">))</span>
<span class="n">encoder_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">'en'</span><span class="p">]</span>
<span class="n">decoder_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">'de'</span><span class="p">]</span>
<span class="n">encoder_optimizer_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">'en_opt'</span><span class="p">]</span>
<span class="n">decoder_optimizer_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">'de_opt'</span><span class="p">]</span>
<span class="n">embedding_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">'embedding'</span><span class="p">]</span>
<span class="n">voc</span> <span class="o">=</span> <span class="n">Voc</span><span class="p">(</span><span class="n">corpus_name</span><span class="p">)</span>
<span class="n">voc</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">'voc_dict'</span><span class="p">]</span>


<span class="k">print</span><span class="p">(</span><span class="s1">'Building encoder and decoder ...'</span><span class="p">)</span>
<span class="c1"># Initialize word embeddings</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="n">embedding</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">embedding_sd</span><span class="p">)</span>
<span class="c1"># Initialize encoder &amp; decoder models</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">EncoderRNN</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">LuongAttnDecoderRNN</span><span class="p">(</span><span class="n">attn_model</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="c1"># Load trained model params</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">encoder_sd</span><span class="p">)</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">decoder_sd</span><span class="p">)</span>
<span class="c1"># Use appropriate device</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># Set dropout layers to eval mode</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Models built and ready to go!'</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Building encoder and decoder ...
Models built and ready to go!
</pre></div>
</div>
</div>
</div>
<div class="section" id="convert-model-to-torch-script">
<h2>Convert Model to Torch Script<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#convert-model-to-torch-script" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<div class="section" id="id4">
<h3>Encoder<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#id4" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h3>
<p>As previously mentioned, to convert the encoder model to Torch Script,
we use <strong>tracing</strong>. Tracing any module requires running an example input
through the model’s <code class="docutils literal notranslate"><span class="pre">forward</span></code> method and trace the computational graph
that the data encounters. The encoder model takes an input sequence and
a corresponding lengths tensor. Therefore, we create an example input
sequence tensor <code class="docutils literal notranslate"><span class="pre">test_seq</span></code>, which is of appropriate size (MAX_LENGTH,
1), contains numbers in the appropriate range
<span class="math notranslate nohighlight"><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x005F;&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-17" style="width: 4.453em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.54em; height: 0px; font-size: 52%;"><span style="position: absolute; clip: rect(1.088em, 1008.42em, 2.77em, -999.994em); top: -2.157em; left: 0em;"><span class="mrow" id="MathJax-Span-18"><span class="mo" id="MathJax-Span-19" style="font-family: MathJax_Main;">[</span><span class="mn" id="MathJax-Span-20" style="font-family: MathJax_Main;">0</span><span class="mo" id="MathJax-Span-21" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-22" style="font-family: MathJax_Math-italic; padding-left: 0.126em;">v</span><span class="mi" id="MathJax-Span-23" style="font-family: MathJax_Math-italic;">o</span><span class="mi" id="MathJax-Span-24" style="font-family: MathJax_Math-italic;">c</span><span class="mo" id="MathJax-Span-25" style="font-family: MathJax_Main;">.</span><span class="mi" id="MathJax-Span-26" style="font-family: MathJax_Math-italic; padding-left: 0.126em;">n</span><span class="mi" id="MathJax-Span-27" style="font-family: MathJax_Math-italic;">u</span><span class="mi" id="MathJax-Span-28" style="font-family: MathJax_Math-italic;">m</span><span class="mi" id="MathJax-Span-29" style="font-family: MathJax_Main;">_</span><span class="mi" id="MathJax-Span-30" style="font-family: MathJax_Math-italic;">w</span><span class="mi" id="MathJax-Span-31" style="font-family: MathJax_Math-italic;">o</span><span class="mi" id="MathJax-Span-32" style="font-family: MathJax_Math-italic;">r</span><span class="mi" id="MathJax-Span-33" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.006em;"></span></span><span class="mi" id="MathJax-Span-34" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-35" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.169em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>v</mi><mi>o</mi><mi>c</mi><mo>.</mo><mi>n</mi><mi>u</mi><mi>m</mi><mi mathvariant="normal">_</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mi>s</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-2">[0, voc.num\_words)</script></span>, and is of the appropriate type (int64). We
also create a <code class="docutils literal notranslate"><span class="pre">test_seq_length</span></code> scalar which realistically contains
the value corresponding to how many words are in the <code class="docutils literal notranslate"><span class="pre">test_seq</span></code>. The
next step is to use the <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code> function to trace the model.
Notice that the first argument we pass is the module that we want to
trace, and the second is a tuple of arguments to the module’s
<code class="docutils literal notranslate"><span class="pre">forward</span></code> method.</p>
</div>
<div class="section" id="id5">
<h3>Decoder<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#id5" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h3>
<p>We perform the same process for tracing the decoder as we did for the
encoder. Notice that we call forward on a set of random inputs to the
traced_encoder to get the output that we need for the decoder. This is
not required, as we could also simply manufacture a tensor of the
correct shape, type, and value range. This method is possible because in
our case we do not have any constraints on the values of the tensors
because we do not have any operations that could fault on out-of-range
inputs.</p>
</div>
<div class="section" id="greedysearchdecoder">
<h3>GreedySearchDecoder<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#greedysearchdecoder" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h3>
<p>Recall that we scripted our searcher module due to the presence of
data-dependent control flow. In the case of scripting, we do the
conversion work up front by adding the decorator and making sure the
implementation complies with scripting rules. We initialize the scripted
searcher the same way that we would initialize an un-scripted variant.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Convert encoder model</span>
<span class="c1"># Create artificial inputs</span>
<span class="n">test_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">MAX_LENGTH</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">)</span>
<span class="n">test_seq_length</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">test_seq</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]])</span>
<span class="c1"># Trace the model</span>
<span class="n">traced_encoder</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="p">(</span><span class="n">test_seq</span><span class="p">,</span> <span class="n">test_seq_length</span><span class="p">))</span>

<span class="c1">### Convert decoder model</span>
<span class="c1"># Create and generate artificial inputs</span>
<span class="n">test_encoder_outputs</span><span class="p">,</span> <span class="n">test_encoder_hidden</span> <span class="o">=</span> <span class="n">traced_encoder</span><span class="p">(</span><span class="n">test_seq</span><span class="p">,</span> <span class="n">test_seq_length</span><span class="p">)</span>
<span class="n">test_decoder_hidden</span> <span class="o">=</span> <span class="n">test_encoder_hidden</span><span class="p">[:</span><span class="n">decoder</span><span class="o">.</span><span class="n">n_layers</span><span class="p">]</span>
<span class="n">test_decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">)</span>
<span class="c1"># Trace the model</span>
<span class="n">traced_decoder</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">decoder</span><span class="p">,</span> <span class="p">(</span><span class="n">test_decoder_input</span><span class="p">,</span> <span class="n">test_decoder_hidden</span><span class="p">,</span> <span class="n">test_encoder_outputs</span><span class="p">))</span>

<span class="c1">### Initialize searcher module</span>
<span class="n">scripted_searcher</span> <span class="o">=</span> <span class="n">GreedySearchDecoder</span><span class="p">(</span><span class="n">traced_encoder</span><span class="p">,</span> <span class="n">traced_decoder</span><span class="p">,</span> <span class="n">decoder</span><span class="o">.</span><span class="n">n_layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="print-graphs">
<h2>Print Graphs<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#print-graphs" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<p>Now that our models are in Torch Script form, we can print the graphs of
each to ensure that we captured the computational graph appropriately.
Since our <code class="docutils literal notranslate"><span class="pre">scripted_searcher</span></code> contains our <code class="docutils literal notranslate"><span class="pre">traced_encoder</span></code> and
<code class="docutils literal notranslate"><span class="pre">traced_decoder</span></code>, these graphs will print inline.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'scripted_searcher graph:</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">scripted_searcher</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>scripted_searcher graph:
 graph(%input_seq : Tensor
      %input_length : Tensor
      %max_length : int
      %3 : Tensor
      %4 : Tensor
      %5 : Tensor
      %6 : Tensor
      %7 : Tensor
      %8 : Tensor
      %9 : Tensor
      %10 : Tensor
      %11 : Tensor
      %12 : Tensor
      %13 : Tensor
      %14 : Tensor
      %15 : Tensor
      %16 : Tensor
      %17 : Tensor
      %18 : Tensor
      %19 : Tensor
      %118 : Tensor
      %119 : Tensor
      %120 : Tensor
      %121 : Tensor
      %122 : Tensor
      %123 : Tensor
      %124 : Tensor
      %125 : Tensor
      %126 : Tensor
      %127 : Tensor
      %128 : Tensor
      %129 : Tensor
      %130 : Tensor) {
  %58 : int = prim::Constant[value=9223372036854775807](), scope: EncoderRNN
  %53 : float = prim::Constant[value=0](), scope: EncoderRNN
  %43 : float = prim::Constant[value=0.1](), scope: EncoderRNN/GRU[gru]
  %42 : int = prim::Constant[value=2](), scope: EncoderRNN/GRU[gru]
  %41 : bool = prim::Constant[value=1](), scope: EncoderRNN/GRU[gru]
  %36 : int = prim::Constant[value=6](), scope: EncoderRNN/GRU[gru]
  %34 : int = prim::Constant[value=500](), scope: EncoderRNN/GRU[gru]
  %25 : int = prim::Constant[value=4](), scope: EncoderRNN
  %24 : Device = prim::Constant[value="cpu"](), scope: EncoderRNN
  %21 : bool = prim::Constant[value=0](), scope: EncoderRNN/Embedding[embedding]
  %20 : int = prim::Constant[value=-1](), scope: EncoderRNN/Embedding[embedding]
  %90 : int = prim::Constant[value=0]()
  %94 : int = prim::Constant[value=1]()
  %input.7 : Float(10, 1, 500) = aten::embedding(%3, %input_seq, %20, %21, %21), scope: EncoderRNN/Embedding[embedding]
  %lengths : Long(1) = aten::to(%input_length, %24, %25, %21, %21), scope: EncoderRNN
  %input.1 : Float(10, 500), %batch_sizes : Long(10) = aten::_pack_padded_sequence(%input.7, %lengths, %21), scope: EncoderRNN
  %35 : int[] = prim::ListConstruct(%25, %94, %34), scope: EncoderRNN/GRU[gru]
  %hx : Float(4, 1, 500) = aten::zeros(%35, %36, %90, %24), scope: EncoderRNN/GRU[gru]
  %40 : Tensor[] = prim::ListConstruct(%4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, %15, %16, %17, %18, %19), scope: EncoderRNN/GRU[gru]
  %46 : Float(10, 1000), %encoder_hidden : Float(4, 1, 500) = aten::gru(%input.1, %batch_sizes, %hx, %40, %41, %42, %43, %21, %41), scope: EncoderRNN/GRU[gru]
  %49 : int = aten::size(%batch_sizes, %90), scope: EncoderRNN
  %max_seq_length : Long() = prim::NumToTensor(%49), scope: EncoderRNN
  %51 : int = prim::TensorToNum(%max_seq_length), scope: EncoderRNN
  %outputs : Float(10, 1, 1000), %55 : Long(1) = aten::_pad_packed_sequence(%46, %batch_sizes, %21, %53, %51), scope: EncoderRNN
  %60 : Float(10, 1, 1000) = aten::slice(%outputs, %90, %90, %58, %94), scope: EncoderRNN
  %65 : Float(10, 1, 1000) = aten::slice(%60, %94, %90, %58, %94), scope: EncoderRNN
  %70 : Float(10, 1!, 500) = aten::slice(%65, %42, %90, %34, %94), scope: EncoderRNN
  %75 : Float(10, 1, 1000) = aten::slice(%outputs, %90, %90, %58, %94), scope: EncoderRNN
  %80 : Float(10, 1, 1000) = aten::slice(%75, %94, %90, %58, %94), scope: EncoderRNN
  %85 : Float(10, 1!, 500) = aten::slice(%80, %42, %34, %58, %94), scope: EncoderRNN
  %encoder_outputs : Float(10, 1, 500) = aten::add(%70, %85, %94), scope: EncoderRNN
  %decoder_hidden.1 : Tensor = aten::slice(%encoder_hidden, %90, %90, %42, %94)
  %98 : int[] = prim::ListConstruct(%94, %94)
  %100 : Tensor = aten::ones(%98, %25, %90, %24)
  %decoder_input.1 : Tensor = aten::mul(%100, %94)
  %103 : int[] = prim::ListConstruct(%90)
  %all_tokens.1 : Tensor = aten::zeros(%103, %25, %90, %24)
  %108 : int[] = prim::ListConstruct(%90)
  %all_scores.1 : Tensor = aten::zeros(%108, %36, %90, %24)
  %all_scores : Tensor, %all_tokens : Tensor, %decoder_hidden : Tensor, %decoder_input : Tensor = prim::Loop(%max_length, %41, %all_scores.1, %all_tokens.1, %decoder_hidden.1, %decoder_input.1)
    block0(%114 : int, %188 : Tensor, %184 : Tensor, %116 : Tensor, %115 : Tensor) {
      %input.2 : Float(1, 1, 500) = aten::embedding(%118, %115, %20, %21, %21), scope: LuongAttnDecoderRNN/Embedding[embedding]
      %input.3 : Float(1, 1, 500) = aten::dropout(%input.2, %43, %21), scope: LuongAttnDecoderRNN/Dropout[embedding_dropout]
      %138 : Tensor[] = prim::ListConstruct(%119, %120, %121, %122, %123, %124, %125, %126), scope: LuongAttnDecoderRNN/GRU[gru]
      %hidden : Float(1, 1, 500), %decoder_hidden.2 : Float(2, 1, 500) = aten::gru(%input.3, %116, %138, %41, %42, %43, %21, %21, %21), scope: LuongAttnDecoderRNN/GRU[gru]
      %147 : Float(10, 1, 500) = aten::mul(%hidden, %encoder_outputs), scope: LuongAttnDecoderRNN/Attn[attn]
      %149 : int[] = prim::ListConstruct(%42), scope: LuongAttnDecoderRNN/Attn[attn]
      %attn_energies : Float(10, 1) = aten::sum(%147, %149, %21), scope: LuongAttnDecoderRNN/Attn[attn]
      %input.4 : Float(1!, 10) = aten::t(%attn_energies), scope: LuongAttnDecoderRNN/Attn[attn]
      %154 : Float(1, 10) = aten::softmax(%input.4, %94), scope: LuongAttnDecoderRNN/Attn[attn]
      %attn_weights : Float(1, 1, 10) = aten::unsqueeze(%154, %94), scope: LuongAttnDecoderRNN/Attn[attn]
      %159 : Float(1!, 10, 500) = aten::transpose(%encoder_outputs, %90, %94), scope: LuongAttnDecoderRNN
      %context.1 : Float(1, 1, 500) = aten::bmm(%attn_weights, %159), scope: LuongAttnDecoderRNN
      %rnn_output : Float(1, 500) = aten::squeeze(%hidden, %90), scope: LuongAttnDecoderRNN
      %context : Float(1, 500) = aten::squeeze(%context.1, %94), scope: LuongAttnDecoderRNN
      %165 : Tensor[] = prim::ListConstruct(%rnn_output, %context), scope: LuongAttnDecoderRNN
      %input.5 : Float(1, 1000) = aten::cat(%165, %94), scope: LuongAttnDecoderRNN
      %168 : Float(1000!, 500!) = aten::t(%127), scope: LuongAttnDecoderRNN/Linear[concat]
      %171 : Float(1, 500) = aten::addmm(%128, %input.5, %168, %94, %94), scope: LuongAttnDecoderRNN/Linear[concat]
      %input.6 : Float(1, 500) = aten::tanh(%171), scope: LuongAttnDecoderRNN
      %173 : Float(500!, 7826!) = aten::t(%129), scope: LuongAttnDecoderRNN/Linear[out]
      %input : Float(1, 7826) = aten::addmm(%130, %input.6, %173, %94, %94), scope: LuongAttnDecoderRNN/Linear[out]
      %decoder_output : Float(1, 7826) = aten::softmax(%input, %94), scope: LuongAttnDecoderRNN
      %decoder_scores : Tensor, %decoder_input.2 : Tensor = aten::max(%decoder_output, %94, %21)
      %186 : Tensor[] = prim::ListConstruct(%184, %decoder_input.2)
      %all_tokens.2 : Tensor = aten::cat(%186, %90)
      %190 : Tensor[] = prim::ListConstruct(%188, %decoder_scores)
      %all_scores.2 : Tensor = aten::cat(%190, %90)
      %decoder_input.3 : Tensor = aten::unsqueeze(%decoder_input.2, %90)
      -&gt; (%41, %all_scores.2, %all_tokens.2, %decoder_hidden.2, %decoder_input.3)
    }
  return (%all_tokens, %all_scores);
}
</pre></div>
</div>
</div>
<div class="section" id="run-evaluation">
<h2>Run Evaluation<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#run-evaluation" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<p>Finally, we will run evaluation of the chatbot model using the Torch
Script models. If converted correctly, the models will behave exactly as
they would in their eager-mode representation.</p>
<p>By default, we evaluate a few common query sentences. If you want to
chat with the bot yourself, uncomment the <code class="docutils literal notranslate"><span class="pre">evaluateInput</span></code> line and
give it a spin.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate examples</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"hello"</span><span class="p">,</span> <span class="s2">"what's up?"</span><span class="p">,</span> <span class="s2">"who are you?"</span><span class="p">,</span> <span class="s2">"where am I?"</span><span class="p">,</span> <span class="s2">"where are you from?"</span><span class="p">]</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
    <span class="n">evaluateExample</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">traced_encoder</span><span class="p">,</span> <span class="n">traced_decoder</span><span class="p">,</span> <span class="n">scripted_searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">)</span>

<span class="c1"># Evaluate your input</span>
<span class="c1">#evaluateInput(traced_encoder, traced_decoder, scripted_searcher, voc)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&gt; hello
Bot: hello .
&gt; what's up?
Bot: i m going to get my car .
&gt; who are you?
Bot: i m the owner .
&gt; where am I?
Bot: in the house .
&gt; where are you from?
Bot: south america .
</pre></div>
</div>
</div>
<div class="section" id="save-model">
<h2>Save Model<a class="anchorjs-link " href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#save-model" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em/1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<p>Now that we have successfully converted our model to Torch Script, we
will serialize it for use in a non-Python deployment environment. To do
this, we can simply save our <code class="docutils literal notranslate"><span class="pre">scripted_searcher</span></code> module, as this is
the user-facing interface for running inference against the chatbot
model. When saving a Script module, use script_module.save(PATH) instead
of torch.save(model, PATH).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scripted_searcher</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"scripted_chatbot.pth"</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Total running time of the script:</strong> ( 0 minutes  1.127 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-deploy-seq2seq-hybrid-frontend-tutorial-py">
<div class="sphx-glr-download docutils container">
<a class="reference download internal has-code" download="" href="https://pytorch.org/tutorials/_downloads/6dce8206a711b28b2b916bfd7de16bbc/deploy_seq2seq_hybrid_frontend_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">deploy_seq2seq_hybrid_frontend_tutorial.py</span></code></a></div>
<div class="sphx-glr-download docutils container">
<a class="reference download internal has-code" download="" href="https://pytorch.org/tutorials/_downloads/e7870c00b625a9c7c808f8fa7a88fcab/deploy_seq2seq_hybrid_frontend_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">deploy_seq2seq_hybrid_frontend_tutorial.ipynb</span></code></a></div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.readthedocs.io/">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>
</article>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html" rel="next" title="Saving and Loading Models">Next <img class="next-page" src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/chevron-right-orange.svg"></a>
<a accesskey="p" class="btn btn-neutral" href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html" rel="prev" title="Transfer Learning Tutorial"><img class="previous-page" src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/chevron-right-orange.svg"> Previous</a>
</div>
<hr>
<div role="contentinfo">
<p>
        © Copyright 2017, PyTorch.

    </p>
</div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org/">Read the Docs</a>. 

</footer>
</div>
</div>
<div class="pytorch-content-right" id="pytorch-content-right" style="height: 100%;">
<div class="pytorch-right-menu" id="pytorch-right-menu" style="top: 0px;">
<div class="pytorch-side-scroll" id="pytorch-side-scroll-right" style="height: 677px;">
<ul>
<li><a class="reference internal title-link has-children" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#">Deploying a Seq2Seq Model with the Hybrid Frontend</a><ul>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#what-is-the-hybrid-frontend">What is the Hybrid Frontend?</a></li>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#acknowledgements">Acknowledgements</a></li>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#prepare-environment">Prepare Environment</a></li>
<li><a class="reference internal not-expanded" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#model-overview">Model Overview</a><ul>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#encoder">Encoder</a></li>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#decoder">Decoder</a></li>
</ul>
</li>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#data-handling">Data Handling</a></li>
<li><a class="reference internal not-expanded" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#define-encoder">Define Encoder</a><ul>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#hybrid-frontend-notes">Hybrid Frontend Notes:</a></li>
</ul>
</li>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#define-decoders-attention-module">Define Decoder’s Attention Module</a></li>
<li><a class="reference internal not-expanded" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#define-decoder">Define Decoder</a><ul>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#id1">Hybrid Frontend Notes:</a></li>
</ul>
</li>
<li><a class="reference internal not-expanded" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#define-evaluation">Define Evaluation</a><ul>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#greedy-search-decoder">Greedy Search Decoder</a></li>
<li><a class="reference internal not-expanded" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#id2">Hybrid Frontend Notes:</a><ul>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#changes">Changes:</a></li>
</ul>
</li>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#evaluating-an-input">Evaluating an Input</a></li>
</ul>
</li>
<li><a class="reference internal not-expanded" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#load-pretrained-parameters">Load Pretrained Parameters</a><ul>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#use-hosted-model">Use hosted model</a></li>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#use-your-own-model">Use your own model</a></li>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#id3">Hybrid Frontend Notes:</a></li>
</ul>
</li>
<li><a class="reference internal not-expanded" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#convert-model-to-torch-script">Convert Model to Torch Script</a><ul>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#id4">Encoder</a></li>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#id5">Decoder</a></li>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#greedysearchdecoder">GreedySearchDecoder</a></li>
</ul>
</li>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#print-graphs">Print Graphs</a></li>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#run-evaluation">Run Evaluation</a></li>
<li><a class="reference internal" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#save-model">Save Model</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</section>
</div>
<script data-url_root="../" id="documentation_options" src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/documentation_options.js.下载" type="text/javascript"></script>
<script src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/jquery.js.下载" type="text/javascript"></script>
<script src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/underscore.js.下载" type="text/javascript"></script>
<script src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/doctools.js.下载" type="text/javascript"></script>
<script async="async" src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/MathJax.js.下载" type="text/javascript"></script>
<script src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/popper.min.js.下载" type="text/javascript"></script>
<script src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/bootstrap.min.js.下载" type="text/javascript"></script>
<script src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/theme.js.下载" type="text/javascript"></script>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-2', 'auto');
  ga('send', 'pageview');

</script>
<img alt="" height="1" src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/saved_resource" style="border-style:none;" width="1">
<!-- Begin Footer -->
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4 text-center">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4 text-center">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4 text-center">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="footer-logo-wrapper">
<a class="footer-logo" href="https://pytorch.org/"></a>
</div>
<div class="footer-links-wrapper">
<div class="footer-links-col">
<ul>
<li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
<li><a href="https://pytorch.org/get-started">Get Started</a></li>
<li><a href="https://pytorch.org/features">Features</a></li>
<li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
<li><a href="https://pytorch.org/blog/">Blog</a></li>
<li><a href="https://pytorch.org/resources">Resources</a></li>
</ul>
</div>
<div class="footer-links-col">
<ul>
<li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
<li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
<li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
<li><a href="https://discuss.pytorch.org/" target="_blank">Discuss</a></li>
<li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
<li><a href="https://pytorch.slack.com/" target="_blank">Slack</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
</ul>
</div>
<div class="footer-links-col follow-us-col">
<ul>
<li class="list-title">Follow Us</li>
<li>
<div id="mc_embed_signup">
<form action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&amp;id=91d0dccd39" class="email-subscribe-form validate" id="mc-embedded-subscribe-form" method="post" name="mc-embedded-subscribe-form" novalidate="" target="_blank">
<div class="email-subscribe-form-fields-wrapper" id="mc_embed_signup_scroll">
<div class="mc-field-group">
<label for="mce-EMAIL" style="display:none;">Email Address</label>
<input class="required email" id="mce-EMAIL" name="EMAIL" placeholder="Email Address" type="email" value="">
</div>
<div class="clear" id="mce-responses">
<div class="response" id="mce-error-response" style="display:none"></div>
<div class="response" id="mce-success-response" style="display:none"></div>
</div> <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
<div aria-hidden="true" style="position: absolute; left: -5000px;"><input name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" type="text" value=""></div>
<div class="clear">
<input class="button email-subscribe-button" id="mc-embedded-subscribe" name="subscribe" type="submit" value="">
</div>
</div>
</form>
</div>
</li>
</ul>
<div class="footer-social-icons">
<a class="facebook" href="https://www.facebook.com/pytorch" target="_blank"></a>
<a class="twitter" href="https://twitter.com/pytorch" target="_blank"></a>
</div>
</div>
</div>
</div>
</footer>
<!-- End Footer -->
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="container">
<div class="mobile-main-menu-header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#"></a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li>
<a href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#">Features</a>
</li>
<li>
<a href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html#">Ecosystem</a>
</li>
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li class="active">
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<a href="https://pytorch.org/docs/stable/index.html">Docs</a>
</li>
<li>
<a href="https://pytorch.org/resources">Resources</a>
</li>
<li>
<a href="https://github.com/pytorch/pytorch">Github</a>
</li>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<script src="./Deploying a Seq2Seq Model with the Hybrid Frontend — PyTorch Tutorials 1.0.0.dev20181207 documentation_files/anchor.min.js.下载" type="text/javascript"></script>
<script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_Math-italic, sans-serif;"></div></div></body></html>